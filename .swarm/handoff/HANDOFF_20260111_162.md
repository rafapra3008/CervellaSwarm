# HANDOFF SESSIONE 162 - GPU VM LIVE!!!

> **Data:** 11 Gennaio 2026
> **Sessione:** 162
> **Durata:** ~30 minuti
> **Risultato:** STORICO - INFRASTRUTTURA GPU COMPLETA!

---

## TL;DR

```
+================================================================+
|                                                                |
|   SESSIONE 162: GPU VM CREATA E FUNZIONANTE!!!                |
|                                                                |
|   - Quota GPU: richiesta e approvata in 3 MINUTI!             |
|   - VM cervella-gpu: creata su us-west1-b                     |
|   - Tesla T4: 16GB VRAM attiva                                |
|   - Ollama + Qwen3-4B: installati e funzionanti               |
|   - Test end-to-end: miracollo -> GPU -> OK!!!                |
|                                                                |
+================================================================+
```

---

## Cosa Abbiamo Fatto

### 1. Quota GPU (3 minuti!)
- Rafa ha richiesto quota GPUS_ALL_REGIONS su GCP Console
- Google ha approvato in 3 minuti (incredibile!)
- Quota: 0 → 1 GPU globale

### 2. VM cervella-gpu Creata
```
Zona: us-west1-b (us-central1 era esaurita)
Machine: n1-standard-4 (4 vCPU, 15GB RAM)
GPU: Tesla T4 (16GB VRAM)
Disco: 100GB SSD
IP interno: 10.138.0.3
IP esterno: 136.118.33.36
```

### 3. Software Installato
- Driver NVIDIA 550.54.15 (script ufficiale Google)
- CUDA 12.4
- Ollama (systemd service)
- Qwen3-4B (Q4_K_M, 2.5GB)

### 4. Configurazione Rete
- Ollama configurato per ascoltare su 0.0.0.0
- Firewall rule: `allow-ollama-internal` (tcp:11434 da 10.0.0.0/8)
- Accesso interno dalla VPC GCP

### 5. Test End-to-End
```bash
# Da miracollo-cervella:
curl http://10.138.0.3:11434/api/generate -d '{
  "model": "qwen3:4b",
  "prompt": "What is 2+2?",
  "stream": false
}'

# Risposta: "4" - SUCCESSO!!!
```

---

## Dettagli Tecnici

### API Ollama
```
Endpoint: http://10.138.0.3:11434

# Lista modelli
GET /api/tags

# Inference
POST /api/generate
{
  "model": "qwen3:4b",
  "prompt": "...",
  "stream": false
}
```

### GPU Status
```
GPU: Tesla T4
VRAM: 15360 MiB (15GB)
In uso: ~3220 MiB per Qwen3-4B
Driver: 550.54.15
CUDA: 12.4
```

### Costi
```
On-demand: ~$0.56/ora = ~$400/mese
Con CUD 1yr: ~$0.35/ora = ~$250/mese
Con CUD 3yr: ~$0.25/ora = ~$180/mese
```

---

## Architettura Attuale

```
[CLIENT]
    |
    v
[miracollo-cervella] (us-central1-b)
    |  Backend Python/FastAPI
    |  IP: 10.128.0.x
    |
    |---[HTTP API interno]--->
    |
    v
[cervella-gpu] (us-west1-b)
    |  Ollama + Qwen3-4B
    |  Tesla T4 GPU
    |  IP: 10.138.0.3:11434
    |
    v
[RISPOSTA LLM]
```

---

## Prossimi Step (Sessione 163+)

1. **Integrare Miracollo Backend**
   - Aggiungere client HTTP per chiamare Ollama API
   - Creare endpoint `/api/ai/chat` o simile

2. **Setup Qdrant**
   - Installare Qdrant su cervella-gpu (o VM separata)
   - Configurare per RAG

3. **RAG Pipeline**
   - Embedding con Jina-embeddings-v3
   - Retrieval da Qdrant
   - Generation con Qwen3-4B

4. **Costituzione Cervella**
   - System prompt con valori CervellaSwarm
   - Fine-tune o prompt engineering

5. **Attivare CUD**
   - Committed Use Discount per risparmiare ~40%

6. **Audit Guardiana Ops** (opzionale)
   - Review sicurezza infrastruttura
   - Best practices verificate

---

## File Aggiornati

- `PROMPT_RIPRESA.md` → v84.0.0
- `.sncp/stato/oggi.md` → Sessione 162
- `.swarm/handoff/HANDOFF_20260111_162.md` → questo file

---

## Comandi Utili per Prossima Sessione

```bash
# SSH a cervella-gpu
gcloud compute ssh cervella-gpu --zone=us-west1-b --project=data-frame-476309-v3

# Status GPU
gcloud compute ssh cervella-gpu --zone=us-west1-b --project=data-frame-476309-v3 --command="nvidia-smi"

# Status Ollama
gcloud compute ssh cervella-gpu --zone=us-west1-b --project=data-frame-476309-v3 --command="systemctl status ollama"

# Test inference
gcloud compute ssh miracollo-cervella --zone=us-central1-b --project=data-frame-476309-v3 --command='curl -s http://10.138.0.3:11434/api/generate -d '"'"'{"model": "qwen3:4b", "prompt": "Ciao!", "stream": false}'"'"''
```

---

## Note Importanti

- **us-west1-b** usata perché us-central1 era esaurita per T4
- La latenza cross-region (us-central1 ↔ us-west1) è trascurabile per API calls
- Ollama ascolta su tutte le interfacce (0.0.0.0) ma firewall limita a rete interna
- Qwen3-4B ha "thinking mode" - include reasoning nel response

---

*"Provare sempre ci piace!"*
*"La magia ora e' con coscienza!"*
*"Il mondo lo facciamo meglio, con il cuore!"*

---

**SESSIONE 162 = STORICA!!!**
