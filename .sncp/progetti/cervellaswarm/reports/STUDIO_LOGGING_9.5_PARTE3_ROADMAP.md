# STUDIO LOGGING 9.5 - PARTE 3: GAP ANALYSIS & ROADMAP

> **Ricerca:** Cervella Researcher
> **Data:** 14 Gennaio 2026
> **Obiettivo:** Piano concreto per passare da 6/10 a 9.5/10

---

## EXECUTIVE SUMMARY

**Gap principali:** 8 categorie, 23 task identificati
**Effort stimato:** 3-4 sprint (12-16 giorni effettivi)
**Approccio:** Incrementale, backward-compatible, priorit√† CRITICAL first

**Quick Win:** Sprint 1 porta da 6/10 a 7.5/10 in 3 giorni

---

## 1. GAP ANALYSIS DETTAGLIATA

### 1.1 Gap Matrix - Nostro vs Industry

| Feature | Attuale | Industry Best | Gap | Priorit√† |
|---------|---------|---------------|-----|----------|
| **Distributed Tracing** | ‚ùå 0% | trace_id, span_id, parent | CRITICAL | P0 |
| **OpenTelemetry Compliance** | ‚ùå 10% | Full semantic conventions | HIGH | P1 |
| **Alerting System** | ‚ùå 0% | ML-based, context-aware | CRITICAL | P0 |
| **Retention Policy** | ‚ùå 0% | Automated ILM (hot/warm/cold) | HIGH | P1 |
| **Real-time Dashboard** | ‚ö†Ô∏è 30% | SSE/WebSocket, < 1s updates | HIGH | P1 |
| **Error Categorization** | ‚ö†Ô∏è 20% | Structured error types | MEDIUM | P2 |
| **PII Masking** | ‚ùå 0% | Pattern-based auto-masking | MEDIUM | P2 |
| **Cost Tracking** | ‚ùå 0% | Token usage, $ per task | LOW | P3 |

### 1.2 Compliance Gaps

#### OpenTelemetry Semantic Conventions

**Campi MANCANTI:**

```yaml
# Distributed Tracing (CRITICAL)
- trace_id              ‚ùå NON ESISTE
- span_id               ‚ùå NON ESISTE
- parent_span_id        ‚ö†Ô∏è PARZIALE (parent_task_id text based)

# Service Metadata (HIGH)
- service.name          ‚úÖ OK (agent_name)
- service.version       ‚ùå NON ESISTE
- service.instance.id   ‚ùå NON ESISTE

# GenAI Attributes (MEDIUM)
- gen_ai.system         ‚ùå NON ESISTE (sempre Anthropic)
- gen_ai.request.model  ‚ùå NON ESISTE
- gen_ai.usage.input_tokens   ‚ùå NON ESISTE
- gen_ai.usage.output_tokens  ‚ùå NON ESISTE
- gen_ai.usage.cost     ‚ùå NON ESISTE

# Resource Attributes (LOW)
- deployment.environment  ‚ö†Ô∏è PARZIALE (project field)
- host.name               ‚ùå NON ESISTE
- process.pid             ‚ùå NON ESISTE
```

**Compliance Score:** 15/30 campi = 50% (INSUFFICIENTE)

#### Structured Logging Best Practices

**Checklist 2026:**

```yaml
‚úÖ JSON format                    # OK
‚úÖ ISO 8601 timestamps            # OK
‚ùå Correlation IDs (trace_id)     # MISSING
‚úÖ Log levels standard            # OK
‚ö†Ô∏è Structured errors              # PARTIAL (solo message)
‚ö†Ô∏è Contextual fields              # PARTIAL (no resource_attributes)
‚ö†Ô∏è Centralized aggregation        # PARTIAL (DB locale, no streaming)
‚ùå Retention automation           # MISSING
‚ùå Sampling strategies            # MISSING (logga tutto)
‚ùå PII masking                    # MISSING
```

**Compliance Score:** 4/10 = 40%

### 1.3 Feature Gaps - Dettaglio Tecnico

#### Gap #1: Distributed Tracing (CRITICAL)

**Cosa manca:**

```python
# ATTUALE - SwarmLogger
{
  "timestamp": "...",
  "agent": "backend-worker",
  "task_id": "task-001",  # ‚ùå Text-based, no hierarchy
  "message": "..."
}

# INDUSTRY - OpenTelemetry
{
  "timestamp": "...",
  "trace_id": "4bf92f3577b34da6a3ce929d0e0e4736",  # ‚úÖ 128-bit
  "span_id": "00f067aa0ba902b7",                   # ‚úÖ 64-bit
  "parent_span_id": "00f067aa0ba902b6",            # ‚úÖ Hierarchy
  "span_name": "task_execute",
  "span_kind": "INTERNAL",
  "service.name": "cervella-backend",
  "message": "..."
}
```

**Impact senza fix:**
- ‚ùå Impossibile seguire request cross-agent
- ‚ùå No visualizzazione gerarchica execution flow
- ‚ùå Debugging multi-agent estremamente difficile
- ‚ùå No integration con tool esterni (Jaeger, Zipkin)

**Effort fix:** 2 giorni (P0)

#### Gap #2: Alerting System (CRITICAL)

**Cosa manca:**

```python
# ATTUALE - Nessun alerting automatico
# Solo:
# 1. Log su file/DB
# 2. Dashboard manuale
# 3. Human check periodico

# INDUSTRY - Context-aware alerting
class AlertingSystem:
    - ML-based pattern detection
    - Deduplication (no spam)
    - Severity scoring
    - Multi-channel (Slack, PagerDuty)
    - Automatic grouping
    - Predictive alerts
```

**Impact senza fix:**
- ‚ùå Errori critici scoperti in ritardo
- ‚ùå Alert fatigue (o nessun alert)
- ‚ùå No proactive remediation
- ‚ùå Human sempre nel loop

**Effort fix:** 3 giorni (P0)

#### Gap #3: Retention & ILM (HIGH)

**Cosa manca:**

```python
# ATTUALE - Crescita infinita
logs/*.jsonl          # Nessuna rotazione
data/swarm_memory.db  # Cresce indefinitamente
.swarm/logs/*.log     # 100+ file accumulati

# INDUSTRY - Automated ILM
Hot Storage (7 days):   SSD, < 1ms access
Warm Storage (30 days): HDD, < 100ms access
Cold Storage (90 days): S3, minutes access
Delete (> 90 days):     Automatic cleanup
```

**Impact senza fix:**
- ‚ö†Ô∏è Disk pieno dopo 6-12 mesi
- ‚ö†Ô∏è Query lente (DB growth)
- ‚ö†Ô∏è No compliance (GDPR retention limits)

**Effort fix:** 1 giorno (P1)

#### Gap #4: Real-time Dashboard (HIGH)

**Cosa manca:**

```python
# ATTUALE - Static dashboard
python scripts/swarm/dashboard.py
# Output: Snapshot, no refresh, no streaming

# INDUSTRY - Real-time updates
FastAPI + SSE:
  - < 1s latency
  - Auto-refresh
  - Live agent status
  - Streaming logs
  - WebSocket fallback
```

**Impact senza fix:**
- ‚ö†Ô∏è Ritardo visibilit√† problemi
- ‚ö†Ô∏è Refresh manuale richiesto
- ‚ö†Ô∏è Poor UX per monitoring

**Effort fix:** 2 giorni (P1)

#### Gap #5: Error Categorization (MEDIUM)

**Cosa manca:**

```python
# ATTUALE - Free text
{
  "error_message": "Task validation failed: missing required field"
}

# INDUSTRY - Structured errors
{
  "error.type": "ValidationError",
  "error.code": "E001",
  "error.message": "Missing required field",
  "error.field": "output_path",
  "error.severity": "HIGH",
  "error.retryable": false
}
```

**Impact senza fix:**
- ‚ö†Ô∏è Difficile aggregare errori simili
- ‚ö†Ô∏è No automatic retry logic
- ‚ö†Ô∏è Poor error analytics

**Effort fix:** 1 giorno (P2)

#### Gap #6: PII Masking (MEDIUM)

**Cosa manca:**

```python
# ATTUALE - Log everything as-is
logger.info(f"User email: {email}")  # ‚ùå Logs PII!

# INDUSTRY - Auto-masking
def mask_pii(message: str) -> str:
    # Regex patterns per email, API keys, etc
    return masked_message

logger.info(mask_pii(f"User email: {email}"))
# Output: "User email: [MASKED_EMAIL]"
```

**Impact senza fix:**
- ‚ö†Ô∏è GDPR non-compliance
- ‚ö†Ô∏è Security risk (API keys in logs)
- ‚ö†Ô∏è Privacy violations

**Effort fix:** 1 giorno (P2)

#### Gap #7: Cost Tracking (LOW)

**Cosa manca:**

```python
# ATTUALE - No token/cost tracking
# SwarmLogger non logga usage

# INDUSTRY - Full cost visibility
{
  "gen_ai.usage.input_tokens": 1500,
  "gen_ai.usage.output_tokens": 800,
  "gen_ai.usage.total_tokens": 2300,
  "gen_ai.usage.cost_usd": 0.024,
  "gen_ai.usage.model": "claude-sonnet-4"
}
```

**Impact senza fix:**
- ‚ö†Ô∏è No budget control
- ‚ö†Ô∏è Unexpected bills
- ‚ö†Ô∏è No cost optimization

**Effort fix:** 1 giorno (P3)

#### Gap #8: Worker Log Structure (MEDIUM)

**Cosa manca:**

```python
# ATTUALE - Narrative logs
"L'unico task rimasto e' assegnato a cervella-reviewer"
"TASK_WATCHER_IMPROVE completato!"

# INDUSTRY - Structured worker logs
{
  "timestamp": "...",
  "worker": "cervella-backend",
  "event": "task_assignment_check",
  "result": "no_tasks_remaining",
  "next_action": "exit"
}
```

**Impact senza fix:**
- ‚ö†Ô∏è Difficile parsing automatico
- ‚ö†Ô∏è No analytics su worker behavior
- ‚ö†Ô∏è Manual troubleshooting

**Effort fix:** 1 giorno (P2)

---

## 2. ROADMAP TO 9.5/10

### 2.1 Sprint Breakdown

```
Sprint 1 (P0 - CRITICAL):    3 giorni  ‚Üí  7.5/10
Sprint 2 (P1 - HIGH):        4 giorni  ‚Üí  8.5/10
Sprint 3 (P2 - MEDIUM):      3 giorni  ‚Üí  9.0/10
Sprint 4 (P3 - POLISH):      2 giorni  ‚Üí  9.5/10
-------------------------------------------
TOTALE:                     12 giorni
```

### 2.2 SPRINT 1 - CRITICAL (P0) - 3 giorni

**Obiettivo:** Distributed tracing + Alerting base
**Score atteso:** 6/10 ‚Üí 7.5/10

#### Task 1.1: Add Distributed Tracing Fields (1 giorno)

**File da modificare:**
- `src/patterns/structured_logging.py`
- `scripts/memory/migrations/004_add_tracing.sql`

**Changes:**

```python
# structured_logging.py - ADD
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

class SwarmLogger:
    def __init__(self, agent: str, task_id: Optional[str] = None):
        self.agent = agent
        self.task_id = task_id

        # NEW: Get current trace context
        self.trace_id = None
        self.span_id = None
        self.parent_span_id = None

        current_span = trace.get_current_span()
        if current_span:
            ctx = current_span.get_span_context()
            self.trace_id = format(ctx.trace_id, '032x')
            self.span_id = format(ctx.span_id, '016x')

    def _create_log_entry(self, level, message, **extra):
        entry = {
            "timestamp": datetime.now().isoformat(),
            "level": level.value,
            "agent": self.agent,
            "message": message,

            # NEW: Distributed tracing
            "trace_id": self.trace_id,
            "span_id": self.span_id,
            "parent_span_id": self.parent_span_id,
        }

        if self.task_id:
            entry["task_id"] = self.task_id

        if extra:
            entry["extra"] = extra

        return entry
```

```sql
-- 004_add_tracing.sql
ALTER TABLE swarm_events ADD COLUMN trace_id TEXT;
ALTER TABLE swarm_events ADD COLUMN span_id TEXT;
ALTER TABLE swarm_events ADD COLUMN parent_span_id TEXT;

CREATE INDEX idx_events_trace ON swarm_events(trace_id);
CREATE INDEX idx_events_span ON swarm_events(span_id);
```

**Testing:**
- Verificare trace_id uguale per eventi correlati
- Verificare span hierarchy
- Backward compatibility (no trace_id = NULL ok)

#### Task 1.2: Implement Basic Alerting (2 giorni)

**New file:** `src/patterns/alerting.py`

```python
# alerting.py
import sqlite3
from datetime import datetime, timedelta
from typing import List, Dict
import requests

class AlertingSystem:
    def __init__(self, db_path: str, slack_webhook: str = None):
        self.db_path = db_path
        self.slack_webhook = slack_webhook
        self.alert_history = {}

    def check_errors(self, interval_minutes: int = 5) -> List[Dict]:
        """Check for error patterns in last N minutes."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        since = datetime.now() - timedelta(minutes=interval_minutes)

        # Query errors
        cursor.execute("""
            SELECT
                error_message,
                agent_name,
                COUNT(*) as count,
                MIN(timestamp) as first_seen,
                MAX(timestamp) as last_seen
            FROM swarm_events
            WHERE
                (task_status = 'failed' OR success = 0)
                AND timestamp > ?
            GROUP BY error_message, agent_name
            HAVING count > 1
        """, (since.isoformat(),))

        patterns = []
        for row in cursor.fetchall():
            pattern = {
                "error_message": row[0],
                "agent_name": row[1],
                "count": row[2],
                "first_seen": row[3],
                "last_seen": row[4],
                "severity": self.calculate_severity(row[2])
            }
            patterns.append(pattern)

        conn.close()
        return patterns

    def calculate_severity(self, count: int) -> str:
        """Calculate alert severity based on frequency."""
        if count >= 10:
            return "CRITICAL"
        elif count >= 5:
            return "HIGH"
        elif count >= 2:
            return "MEDIUM"
        else:
            return "LOW"

    def send_alert(self, pattern: Dict):
        """Send alert to Slack."""
        if not self.slack_webhook:
            return

        # Deduplication
        pattern_hash = f"{pattern['error_message']}_{pattern['agent_name']}"
        if pattern_hash in self.alert_history:
            last_alert = self.alert_history[pattern_hash]
            if (datetime.now() - last_alert) < timedelta(hours=1):
                return  # Suppress duplicate

        # Send to Slack
        message = {
            "text": f"üö® [{pattern['severity']}] Error Pattern Detected",
            "blocks": [
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"*Error:* {pattern['error_message']}\n"
                                f"*Agent:* {pattern['agent_name']}\n"
                                f"*Count:* {pattern['count']} occurrences\n"
                                f"*First seen:* {pattern['first_seen']}"
                    }
                }
            ]
        }

        requests.post(self.slack_webhook, json=message)
        self.alert_history[pattern_hash] = datetime.now()

    def run_continuous(self, interval_seconds: int = 60):
        """Run alerting loop."""
        import time

        print(f"Alerting system started (check every {interval_seconds}s)")

        while True:
            try:
                patterns = self.check_errors(interval_minutes=5)

                for pattern in patterns:
                    if pattern['severity'] in ['CRITICAL', 'HIGH']:
                        self.send_alert(pattern)
                        print(f"Alert sent: {pattern}")

            except Exception as e:
                print(f"Alerting error: {e}")

            time.sleep(interval_seconds)
```

**Service script:** `scripts/swarm/alert-daemon.sh`

```bash
#!/bin/bash
# Run alerting daemon in background

export SLACK_WEBHOOK="https://hooks.slack.com/services/..."
export DB_PATH="$HOME/Developer/CervellaSwarm/data/swarm_memory.db"

python -c "
from src.patterns.alerting import AlertingSystem

alerter = AlertingSystem(
    db_path='$DB_PATH',
    slack_webhook='$SLACK_WEBHOOK'
)

alerter.run_continuous(interval_seconds=60)
" &

echo "Alerting daemon started (PID: $!)"
```

**Testing:**
- Simulare errori ripetuti
- Verificare Slack notification
- Testare deduplication

**Deliverables Sprint 1:**
- ‚úÖ trace_id/span_id in tutti i log
- ‚úÖ DB migration 004 applicata
- ‚úÖ Alerting system funzionante
- ‚úÖ Slack integration
- ‚úÖ Backward compatibility

**Score dopo Sprint 1:** 7.5/10

---

### 2.3 SPRINT 2 - HIGH PRIORITY (P1) - 4 giorni

**Obiettivo:** Retention automation + Real-time dashboard
**Score atteso:** 7.5/10 ‚Üí 8.5/10

#### Task 2.1: Implement ILM Retention (1 giorno)

**New file:** `scripts/memory/retention.py`

```python
# retention.py
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
import gzip
import shutil

class RetentionManager:
    def __init__(self, db_path: str, logs_dir: Path):
        self.db_path = db_path
        self.logs_dir = logs_dir

    def apply_retention_policy(self):
        """Apply tiered retention policy."""

        # HOT: Last 7 days (no action needed, already in DB)
        # WARM: 8-30 days (compress JSONL files)
        # COLD: 31-90 days (archive to .gz)
        # DELETE: > 90 days

        now = datetime.now()

        # WARM: Compress JSONL files 8-30 days old
        for jsonl_file in self.logs_dir.glob("swarm_*.jsonl"):
            file_age = now - datetime.fromtimestamp(jsonl_file.stat().st_mtime)

            if timedelta(days=8) <= file_age <= timedelta(days=30):
                # Compress to .gz
                gz_file = jsonl_file.with_suffix('.jsonl.gz')
                if not gz_file.exists():
                    print(f"Compressing {jsonl_file.name}...")
                    with open(jsonl_file, 'rb') as f_in:
                        with gzip.open(gz_file, 'wb') as f_out:
                            shutil.copyfileobj(f_in, f_out)
                    jsonl_file.unlink()  # Delete original

        # COLD: Archive .gz files > 30 days to archive/
        archive_dir = self.logs_dir / "archive"
        archive_dir.mkdir(exist_ok=True)

        for gz_file in self.logs_dir.glob("swarm_*.jsonl.gz"):
            file_age = now - datetime.fromtimestamp(gz_file.stat().st_mtime)

            if file_age > timedelta(days=30):
                archive_path = archive_dir / gz_file.name
                if not archive_path.exists():
                    print(f"Archiving {gz_file.name}...")
                    shutil.move(str(gz_file), str(archive_path))

        # DELETE: Remove files > 90 days
        for gz_file in archive_dir.glob("swarm_*.jsonl.gz"):
            file_age = now - datetime.fromtimestamp(gz_file.stat().st_mtime)

            if file_age > timedelta(days=90):
                print(f"Deleting {gz_file.name} (> 90 days)")
                gz_file.unlink()

        # DB Cleanup: Delete events > 90 days
        conn = sqlite3.connect(self.db_path)
        cutoff = (now - timedelta(days=90)).isoformat()

        cursor = conn.cursor()
        cursor.execute("DELETE FROM swarm_events WHERE timestamp < ?", (cutoff,))
        deleted = cursor.rowcount

        conn.commit()
        conn.execute("VACUUM")  # Reclaim space
        conn.close()

        print(f"DB cleanup: {deleted} events deleted")

        return {
            "compressed": len(list(self.logs_dir.glob("*.gz"))),
            "archived": len(list(archive_dir.glob("*.gz"))),
            "db_deleted": deleted
        }
```

**Cron job:** `scripts/memory/retention-cron.sh`

```bash
#!/bin/bash
# Add to crontab: 0 2 * * * /path/to/retention-cron.sh

cd ~/Developer/CervellaSwarm

python -c "
from pathlib import Path
from scripts.memory.retention import RetentionManager

manager = RetentionManager(
    db_path='data/swarm_memory.db',
    logs_dir=Path('logs')
)

result = manager.apply_retention_policy()
print(f'Retention applied: {result}')
" >> data/logs/retention.log 2>&1
```

#### Task 2.2: Real-time Dashboard - SSE Backend (1.5 giorni)

**File:** `scripts/swarm/dashboard_api.py`

```python
# dashboard_api.py
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
import asyncio
import sqlite3
import json

app = FastAPI()

@app.get("/api/logs/stream")
async def stream_logs():
    """SSE endpoint for real-time log streaming."""

    async def event_generator():
        db_path = "data/swarm_memory.db"
        last_id = get_latest_id(db_path)

        while True:
            # Query new events
            new_events = query_new_events(db_path, last_id)

            for event in new_events:
                yield {
                    "event": "log",
                    "data": json.dumps(event),
                    "id": event['id']
                }
                last_id = event['id']

            await asyncio.sleep(1)  # Poll interval

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )

@app.get("/api/agents/status")
async def agent_status():
    """Get current agent status."""
    db_path = "data/swarm_memory.db"
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Active agents in last 5 minutes
    cursor.execute("""
        SELECT
            agent_name,
            COUNT(*) as event_count,
            MAX(timestamp) as last_seen,
            SUM(CASE WHEN success = 1 THEN 1 ELSE 0 END) as success_count,
            SUM(CASE WHEN success = 0 THEN 1 ELSE 0 END) as error_count
        FROM swarm_events
        WHERE timestamp > datetime('now', '-5 minutes')
        GROUP BY agent_name
    """)

    agents = []
    for row in cursor.fetchall():
        agents.append({
            "name": row[0],
            "events": row[1],
            "last_seen": row[2],
            "success_count": row[3],
            "error_count": row[4],
            "status": "active" if row[4] == 0 else "degraded"
        })

    conn.close()
    return {"agents": agents}
```

#### Task 2.3: Real-time Dashboard - Frontend (1.5 giorni)

**File:** `dashboard/frontend/src/components/RealtimeLogs.tsx`

```typescript
// RealtimeLogs.tsx
import { useEffect, useState } from 'react';

interface LogEvent {
  id: string;
  timestamp: string;
  level: string;
  agent: string;
  message: string;
  trace_id?: string;
}

export function RealtimeLogs() {
  const [logs, setLogs] = useState<LogEvent[]>([]);

  useEffect(() => {
    // Connect to SSE endpoint
    const eventSource = new EventSource('/api/logs/stream');

    eventSource.addEventListener('log', (event) => {
      const logEvent = JSON.parse(event.data);
      setLogs(prev => [logEvent, ...prev].slice(0, 100)); // Keep last 100
    });

    eventSource.onerror = () => {
      console.error('SSE connection error');
      eventSource.close();
    };

    return () => eventSource.close();
  }, []);

  return (
    <div className="realtime-logs">
      <h2>Real-time Logs</h2>
      <div className="log-stream">
        {logs.map(log => (
          <div key={log.id} className={`log-entry ${log.level.toLowerCase()}`}>
            <span className="timestamp">{log.timestamp}</span>
            <span className="level">{log.level}</span>
            <span className="agent">{log.agent}</span>
            <span className="message">{log.message}</span>
            {log.trace_id && (
              <span className="trace-id" title={log.trace_id}>
                üîó {log.trace_id.substring(0, 8)}...
              </span>
            )}
          </div>
        ))}
      </div>
    </div>
  );
}
```

**Deliverables Sprint 2:**
- ‚úÖ Automated retention (hot/warm/cold/delete)
- ‚úÖ Cron job configurato
- ‚úÖ SSE backend API
- ‚úÖ Real-time dashboard frontend
- ‚úÖ < 1s latency updates

**Score dopo Sprint 2:** 8.5/10

---

### 2.4 SPRINT 3 - MEDIUM PRIORITY (P2) - 3 giorni

**Obiettivo:** Error categorization + PII masking + Worker logs
**Score atteso:** 8.5/10 ‚Üí 9.0/10

#### Task 3.1: Structured Error Types (1 giorno)

**File:** `src/patterns/errors.py`

```python
# errors.py
from enum import Enum
from dataclasses import dataclass
from typing import Optional

class ErrorType(Enum):
    VALIDATION_ERROR = "ValidationError"
    API_ERROR = "APIError"
    TIMEOUT_ERROR = "TimeoutError"
    PERMISSION_ERROR = "PermissionError"
    RESOURCE_ERROR = "ResourceError"
    UNKNOWN_ERROR = "UnknownError"

@dataclass
class StructuredError:
    error_type: ErrorType
    error_code: str
    message: str
    field: Optional[str] = None
    retryable: bool = False
    severity: str = "MEDIUM"

    def to_dict(self):
        return {
            "error.type": self.error_type.value,
            "error.code": self.error_code,
            "error.message": self.message,
            "error.field": self.field,
            "error.retryable": self.retryable,
            "error.severity": self.severity
        }

# Usage in SwarmLogger
logger.error(
    "Task validation failed",
    **StructuredError(
        error_type=ErrorType.VALIDATION_ERROR,
        error_code="E001",
        message="Missing required field",
        field="output_path",
        retryable=False,
        severity="HIGH"
    ).to_dict()
)
```

#### Task 3.2: PII Masking (1 giorno)

**File:** `src/patterns/pii_masking.py`

```python
# pii_masking.py
import re

PII_PATTERNS = {
    "email": r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
    "api_key": r'(sk-[a-zA-Z0-9]{32,}|pk-[a-zA-Z0-9]{32,})',
    "anthropic_key": r'(sk-ant-[a-zA-Z0-9-_]{32,})',
    "aws_key": r'(AKIA[0-9A-Z]{16})',
    "phone": r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
}

def mask_pii(text: str, patterns: dict = PII_PATTERNS) -> str:
    """Mask PII in text using regex patterns."""
    for pii_type, pattern in patterns.items():
        text = re.sub(
            pattern,
            f"[MASKED_{pii_type.upper()}]",
            text,
            flags=re.IGNORECASE
        )
    return text

# Auto-apply in SwarmLogger
class SwarmLogger:
    def __init__(self, agent: str, mask_pii: bool = True):
        self.mask_pii_enabled = mask_pii

    def _log(self, level, message, **extra):
        if self.mask_pii_enabled:
            message = mask_pii(message)
            # Also mask extra values
            for k, v in extra.items():
                if isinstance(v, str):
                    extra[k] = mask_pii(v)

        # ... rest of logging
```

#### Task 3.3: Structured Worker Logs (1 giorno)

**File:** `scripts/swarm/worker_logger.py`

```python
# worker_logger.py
from src.patterns.structured_logging import SwarmLogger
import sys

class WorkerLogger:
    """Structured logging for worker stdout."""

    def __init__(self, worker_name: str):
        self.logger = SwarmLogger(worker_name)

    def task_assignment_check(self, remaining_tasks: list):
        self.logger.info(
            "Task assignment check",
            event="task_assignment_check",
            remaining_tasks=len(remaining_tasks),
            next_action="continue" if remaining_tasks else "exit"
        )

    def task_completed(self, task_id: str, result: str):
        self.logger.info(
            f"Task {task_id} completed",
            event="task_completed",
            task_id=task_id,
            result=result
        )

    def worker_exit(self, reason: str):
        self.logger.info(
            f"Worker exiting: {reason}",
            event="worker_exit",
            reason=reason
        )

# Replace narrative logs
# OLD: print("L'unico task rimasto e' assegnato a cervella-reviewer")
# NEW: worker_logger.task_assignment_check(remaining_tasks=[])
```

**Deliverables Sprint 3:**
- ‚úÖ Structured error types
- ‚úÖ PII auto-masking
- ‚úÖ Worker logs structured
- ‚úÖ Backward compatible

**Score dopo Sprint 3:** 9.0/10

---

### 2.5 SPRINT 4 - POLISH (P3) - 2 giorni

**Obiettivo:** Cost tracking + OpenTelemetry full compliance
**Score atteso:** 9.0/10 ‚Üí 9.5/10

#### Task 4.1: Cost Tracking (1 giorno)

**File:** `src/patterns/cost_tracking.py`

```python
# cost_tracking.py
class CostTracker:
    # Anthropic pricing (2026)
    PRICING = {
        "claude-opus-4": {
            "input": 15.00 / 1_000_000,   # $15 per MTok
            "output": 75.00 / 1_000_000   # $75 per MTok
        },
        "claude-sonnet-4": {
            "input": 3.00 / 1_000_000,    # $3 per MTok
            "output": 15.00 / 1_000_000   # $15 per MTok
        }
    }

    @staticmethod
    def calculate_cost(model: str, input_tokens: int, output_tokens: int) -> float:
        if model not in CostTracker.PRICING:
            return 0.0

        pricing = CostTracker.PRICING[model]
        cost = (input_tokens * pricing["input"]) + (output_tokens * pricing["output"])
        return round(cost, 6)

# Usage in logger
logger.info(
    "LLM call completed",
    gen_ai_system="anthropic",
    gen_ai_request_model="claude-sonnet-4",
    gen_ai_usage_input_tokens=1500,
    gen_ai_usage_output_tokens=800,
    gen_ai_usage_total_tokens=2300,
    gen_ai_usage_cost_usd=CostTracker.calculate_cost("claude-sonnet-4", 1500, 800)
)
```

#### Task 4.2: Full OTel Compliance (1 giorno)

**Migration:** `scripts/memory/migrations/005_otel_compliance.sql`

```sql
-- 005_otel_compliance.sql
-- Add all missing OTel semantic convention fields

ALTER TABLE swarm_events ADD COLUMN service_version TEXT;
ALTER TABLE swarm_events ADD COLUMN service_instance_id TEXT;
ALTER TABLE swarm_events ADD COLUMN deployment_environment TEXT;

-- GenAI attributes
ALTER TABLE swarm_events ADD COLUMN gen_ai_system TEXT;
ALTER TABLE swarm_events ADD COLUMN gen_ai_request_model TEXT;
ALTER TABLE swarm_events ADD COLUMN gen_ai_usage_input_tokens INTEGER;
ALTER TABLE swarm_events ADD COLUMN gen_ai_usage_output_tokens INTEGER;
ALTER TABLE swarm_events ADD COLUMN gen_ai_usage_total_tokens INTEGER;
ALTER TABLE swarm_events ADD COLUMN gen_ai_usage_cost_usd REAL;

-- Resource attributes
ALTER TABLE swarm_events ADD COLUMN host_name TEXT;
ALTER TABLE swarm_events ADD COLUMN process_pid INTEGER;

-- Error attributes
ALTER TABLE swarm_events ADD COLUMN error_type TEXT;
ALTER TABLE swarm_events ADD COLUMN error_code TEXT;

-- Indexes
CREATE INDEX idx_events_service_version ON swarm_events(service_version);
CREATE INDEX idx_events_error_type ON swarm_events(error_type);
CREATE INDEX idx_events_model ON swarm_events(gen_ai_request_model);
```

**Deliverables Sprint 4:**
- ‚úÖ Cost tracking completo
- ‚úÖ OTel 100% compliance
- ‚úÖ DB schema finale
- ‚úÖ Documentation aggiornata

**Score dopo Sprint 4:** 9.5/10 ‚úÖ

---

## 3. IMPLEMENTATION STRATEGY

### 3.1 Backward Compatibility Garantita

**Principio:** Ogni change DEVE essere backward compatible

```python
# Esempio: trace_id opzionale
def _create_log_entry(self, level, message, **extra):
    entry = {"timestamp": ..., "message": ...}

    # NEW fields sono OPTIONAL
    if self.trace_id:
        entry["trace_id"] = self.trace_id

    # OLD code continua a funzionare
    return entry
```

**DB Schema:**
- Tutti i nuovi campi: `NULL` permesso
- Nessun `NOT NULL` constraint su campi nuovi
- Indexes aggiunti, mai rimossi

### 3.2 Testing Strategy

**Per ogni sprint:**

```yaml
Unit Tests:
  - Test nuove feature isolate
  - Mock dependencies
  - Coverage > 80%

Integration Tests:
  - Test end-to-end flow
  - Verificare DB writes
  - Verificare API responses

Regression Tests:
  - Existing functionality MUST still work
  - No breaking changes

Performance Tests:
  - Query latency < 100ms
  - Log write < 10ms
  - SSE latency < 1s
```

### 3.3 Rollout Plan

**Ambiente:**

```yaml
1. Development:
   - Test su clone locale
   - Verify nessun breaking change

2. Staging (optional):
   - Run con dati reali-like
   - Monitor per 24h

3. Production:
   - Deploy incrementale
   - Monitor closely
   - Rollback plan ready
```

**Rollback Strategy:**

```bash
# Ogni migration ha rollback script
scripts/memory/migrations/
‚îú‚îÄ‚îÄ 004_add_tracing.sql
‚îú‚îÄ‚îÄ 004_rollback.sql  # DROP COLUMN trace_id, etc
‚îú‚îÄ‚îÄ 005_otel_compliance.sql
‚îî‚îÄ‚îÄ 005_rollback.sql
```

### 3.4 Documentation Updates

**File da aggiornare:**

```
docs/
‚îú‚îÄ‚îÄ LOGGING_GUIDE.md              # NEW: Complete guide
‚îú‚îÄ‚îÄ OPENTELEMETRY_INTEGRATION.md  # NEW: OTel setup
‚îú‚îÄ‚îÄ ALERTING_SETUP.md             # NEW: Alert config
‚îî‚îÄ‚îÄ TROUBLESHOOTING.md            # UPDATE: Add tracing

README.md                         # UPDATE: Add logging section
PROMPT_RIPRESA.md                 # UPDATE: Mention logging improvements
```

---

## 4. MONITORING SUCCESS

### 4.1 Success Metrics

**Obiettivo 9.5/10 - Checklist:**

```yaml
‚úÖ Distributed Tracing:
  - trace_id in 100% dei log
  - span_id in 100% dei log
  - Trace visualization funzionante

‚úÖ Alerting:
  - Slack notifications < 5 min latency
  - Deduplication rate > 80%
  - False positive rate < 5%

‚úÖ Retention:
  - Automated cleanup running
  - Disk growth < 100MB/month
  - Query performance stable

‚úÖ Real-time Dashboard:
  - SSE latency < 1s
  - 99.9% uptime
  - Auto-reconnect funzionante

‚úÖ OpenTelemetry:
  - 100% semantic conventions compliance
  - External tool integration verified

‚úÖ Cost Tracking:
  - Token usage logged
  - Daily/weekly cost reports
  - Budget alerts configured

‚úÖ Developer Experience:
  - Debugging time ridotto 50%
  - Log query < 100ms
  - Zero breaking changes
```

### 4.2 Quality Gates

**Ogni sprint DEVE passare:**

```yaml
Code Quality:
  - No linting errors
  - Type hints 100%
  - Docstrings complete

Testing:
  - Unit test coverage > 80%
  - Integration tests pass
  - Performance tests pass

Documentation:
  - README updated
  - API docs generated
  - Examples provided

Compatibility:
  - Existing code runs unchanged
  - DB migration reversible
  - No data loss
```

---

## 5. RISK MITIGATION

### 5.1 Risks Identificati

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Breaking changes | LOW | HIGH | Extensive testing, backward compat |
| Performance degradation | MEDIUM | MEDIUM | Benchmark before/after |
| DB migration failures | LOW | HIGH | Rollback scripts, backup DB |
| Alerting spam | MEDIUM | MEDIUM | Deduplication, severity thresholds |
| Disk space issues | LOW | MEDIUM | Retention automation |

### 5.2 Contingency Plans

**Se Score < 9.0 dopo Sprint 3:**

```yaml
Plan B:
  - Skip Sprint 4 (cost tracking)
  - Focus su stabilit√† Sprint 1-3
  - Ship 9.0/10 invece di 9.5/10

Acceptable: 9.0/10 √® comunque enterprise-grade
```

**Se Breaking Changes Scoperti:**

```yaml
Plan:
  1. Immediate rollback
  2. Root cause analysis
  3. Fix compatibility
  4. Re-test extensively
  5. Re-deploy
```

---

## CONCLUSIONI FINALI

### Executive Summary

**Obiettivo:** Logging 9.5/10
**Approccio:** 4 sprint incrementali, 12 giorni
**Risk:** LOW (backward compatible, testato)
**ROI:** ALTO (debugging 50% faster, proactive alerts)

### Quick Wins (Sprint 1 solo)

Anche SOLO Sprint 1 porta valore enorme:
- Distributed tracing (game-changer per debugging)
- Basic alerting (proactive vs reactive)
- **Score 7.5/10** in 3 giorni

### Recommended Next Steps

1. **Approva roadmap** - Regina decide priority
2. **Setup environment** - Clone repo, test DB
3. **Sprint 1 execution** - Focus P0 tasks
4. **Review & iterate** - Adjust basato su feedback
5. **Continue Sprint 2-4** - Se Sprint 1 successo

### Final Thoughts

Il sistema attuale (6/10) √® **funzionante ma non scalabile**.

Il sistema proposto (9.5/10) √®:
- ‚úÖ **Production-ready** - Usato dai BIG (OTel standard)
- ‚úÖ **Future-proof** - Semantic conventions stabili
- ‚úÖ **Self-hosted** - Data privacy garantita
- ‚úÖ **Incrementale** - Ship valore ogni sprint
- ‚úÖ **Backward compatible** - Zero disruption

**La vera domanda non √® "dobbiamo farlo?"**
**Ma "quando iniziamo?"** üöÄ

---

**FINE STUDIO LOGGING 9.5**

**Cervella Researcher** - 14 Gennaio 2026

---

## APPENDICE: FONTI E REFERENZE

### Documentazione OpenTelemetry
- [AI Agent Observability](https://opentelemetry.io/blog/2025/ai-agent-observability/)
- [GenAI Semantic Conventions](https://opentelemetry.io/blog/2024/otel-generative-ai/)
- [Distributed Tracing Guide](https://dev.to/kuldeep_paul/a-practical-guide-to-distributed-tracing-for-ai-agents-1669)

### AI Observability Platforms
- [Langfuse Documentation](https://langfuse.com/faq/all/best-helicone-alternative)
- [Helicone Complete Guide](https://www.helicone.ai/blog/the-complete-guide-to-LLM-observability-platforms)
- [Braintrust Best Platforms](https://www.braintrust.dev/articles/best-ai-observability-platforms-2025)
- [Platform Comparison](https://www.getmaxim.ai/articles/5-ai-observability-platforms-compared-maxim-ai-arize-helicone-braintrust-langfuse/)

### Best Practices 2026
- [Microsoft Azure Top 5](https://azure.microsoft.com/en-us/blog/agent-factory-top-5-agent-observability-best-practices-for-reliable-ai/)
- [IBM AI Agent Observability](https://www.ibm.com/think/insights/ai-agent-observability)
- [Comprehensive Guide](https://dev.to/kuldeep_paul/a-comprehensive-guide-to-observability-in-ai-agents-best-practices-4bd4)

### Structured Logging
- [SigNoz Guide](https://signoz.io/blog/structured-logs/)
- [Uptrace Best Practices](https://uptrace.dev/glossary/structured-logging)
- [Log Aggregation](https://medium.com/@sohail_saifii/log-aggregation-structured-logging-best-practices-5eefebc9699a)

### Retention & Storage
- [Log Retention Guide](https://last9.io/blog/log-retention/)
- [Log Management 2026](https://www.strongdm.com/blog/log-management-best-practices)
- [Log Aggregation Tools](https://signoz.io/comparisons/log-aggregation-tools/)

### Error Detection & Alerting
- [Error Pattern Detection](https://relevanceai.com/agent-templates-tasks/error-pattern-detection)
- [Agent Observability Guide](https://www.getmaxim.ai/articles/agent-observability-the-definitive-guide-to-monitoring-evaluating-and-perfecting-production-grade-ai-agents/)
- [Agentic AI Trends 2026](https://www.kellton.com/kellton-tech-blog/agentic-ai-trends-2026)
