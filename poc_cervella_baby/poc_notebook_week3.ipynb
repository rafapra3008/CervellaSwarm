{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cervella Baby POC - Week 3 (FINALE)\n",
    "\n",
    "> **TIER 3 - Complex Tasks (T19-T20) + GO/NO-GO Decision**\n",
    "\n",
    "---\n",
    "\n",
    "## Stato POC\n",
    "\n",
    "| Week | Task | Risultato | Score |\n",
    "|------|------|-----------|-------|\n",
    "| Week 1 | T01-T10 (Simple) | PASS 9/10 | ~85% |\n",
    "| Week 2 | T11-T18 (Medium) | PASS 8/8 | 89.4% |\n",
    "| **Week 3** | **T19-T20 (Complex)** | **IN CORSO** | **?** |\n",
    "\n",
    "---\n",
    "\n",
    "## About Week 3\n",
    "\n",
    "| Item | Value |\n",
    "|------|-------|\n",
    "| **Task** | T19-T20 (TIER 3 - Complex) |\n",
    "| **Obiettivo** | Documentare GAP su task complessi |\n",
    "| **Threshold** | 70% score (ma non richiesto per PASS POC) |\n",
    "| **GO/NO-GO** | 1 Febbraio 2026 |\n",
    "\n",
    "---\n",
    "\n",
    "**NOTA IMPORTANTE:**\n",
    "I task TIER 3 servono a **documentare i limiti** del modello.\n",
    "Il POC e' gia' PASS con Week 1 + Week 2!\n",
    "\n",
    "---\n",
    "\n",
    "*\"La magia ora e' con coscienza!\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "**IMPORTANTE:** Prima di eseguire:\n",
    "1. Runtime > Change runtime type > **T4 GPU**\n",
    "2. Hai almeno 15GB RAM disponibile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU disponibile\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth (ottimizzato per Colab)\n",
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install altre dipendenze\n",
    "%%capture\n",
    "!pip install transformers datasets accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model - Qwen3-4B-Instruct-2507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model con Unsloth (4-bit quantization)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen3-4B-Instruct-2507\",\n",
    "    max_seq_length=8192,  # Aumentato per task MOLTO complessi\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Enable fast inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"\\nModello caricato!\")\n",
    "print(f\"Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Prompt - COSTITUZIONE Cervella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "# CERVELLA - Core Identity\n",
    "\n",
    "## CHI SONO\n",
    "\n",
    "Sono Cervella, PARTNER STRATEGICO di Rafa (non assistente).\n",
    "\n",
    "**La differenza:**\n",
    "- Assistente: \"Si Rafa, faccio subito\"\n",
    "- Partner: \"Aspetta Rafa, prima devo capire/ricercare/pensare\"\n",
    "\n",
    "**Ruolo:**\n",
    "- Rafa = CEO & Visionary (il PERCHE)\n",
    "- Io = Strategic Partner (il COME)\n",
    "- Insieme = La magia\n",
    "\n",
    "## OBIETTIVO FINALE: LIBERTA GEOGRAFICA\n",
    "\n",
    "Non lavoriamo per il codice. Lavoriamo per la LIBERTA.\n",
    "\n",
    "## FILOSOFIA CORE - I Pilastri:\n",
    "\n",
    "1. \"Lavoriamo in PACE! Senza CASINO! Dipende da NOI!\"\n",
    "2. \"Fatto BENE > Fatto VELOCE\"\n",
    "3. \"I dettagli fanno SEMPRE la differenza\"\n",
    "4. \"Nulla e complesso - solo non ancora studiato!\"\n",
    "5. \"Non e sempre come immaginiamo... ma alla fine e il 100000%!\"\n",
    "\n",
    "## COME LAVORO - LE 4 REGOLE DEL PARTNER\n",
    "\n",
    "1. RAGIONARE - Non eseguire ciecamente\n",
    "2. RICERCARE - Prima di proporre\n",
    "3. DISSENTIRE - Quando necessario\n",
    "4. PROTEGGERE - Il progetto e Rafa\n",
    "\n",
    "## TONE & VOICE\n",
    "\n",
    "- Con CALMA e PRECISIONE\n",
    "- Mai fretta, mai approssimazioni\n",
    "- Ogni dettaglio conta. Sempre.\n",
    "- Output CONCISO e strutturato\n",
    "\n",
    "## REGOLA D'ORO\n",
    "\n",
    "PRIMA DI AGIRE, CHIEDITI:\n",
    "1. Ho CAPITO cosa serve veramente?\n",
    "2. Ho RICERCATO come si fa?\n",
    "3. Ho RAGIONATO sulle conseguenze?\n",
    "4. Sto facendo la cosa GIUSTA o la cosa VELOCE?\n",
    "\n",
    "Se anche UNA risposta e NO -> FERMATI e PENSA\n",
    "\"\"\"\n",
    "\n",
    "print(f\"System prompt length: {len(SYSTEM_PROMPT)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Task Dataset - T19-T20 (TIER 3 - Complex)\n",
    "\n",
    "**NOTA:** Questi task servono a documentare i LIMITI del modello.\n",
    "Non sono richiesti per il PASS del POC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Task Dataset TIER 3 (Complex)\nTASKS = [\n    {\n        \"id\": \"T19\",\n        \"name\": \"Strategic Planning 6 Mesi\",\n        \"input\": \"\"\"Context:\n- OBIETTIVO: Liberta geografica (lavorare da qualsiasi posto)\n- PROGETTI ATTIVI:\n  1. Miracollo (app focus/productivity) - LIVE, revenue ~$50/mese\n  2. CervellaSwarm CLI (multi-agent tool) - In sviluppo\n  3. Cervella Baby (modello open source) - POC in corso\n- CONSTRAINT: Solo Rafa (1 persona), no hiring, $500/mese budget max\n- SITUAZIONE: Rafa lavora full-time, progetti sono side-project\n\nTask: Crea piano strategico 6 mesi (Febbraio - Luglio 2026).\n\nOutput richiesto:\n1. Roadmap mensile con milestone\n2. Allocation tempo (ore/settimana per progetto)\n3. Budget allocation\n4. Risk management (top 3 rischi + mitigation)\n5. Success criteria per ogni progetto\n6. Decision point: quando fare GO/NO-GO per ogni progetto\n\nFilosofia guida: Fatto BENE > Fatto VELOCE\"\"\",\n        \"pass_threshold\": 0.70\n    },\n    {\n        \"id\": \"T20\",\n        \"name\": \"Architettura Major Decision - SNCP Cross-Project\",\n        \"input\": \"\"\"Context: SNCP (Sistema Nervoso Centrale Progetti) deve supportare memoria condivisa cross-project.\n\nSituazione attuale:\n- Ogni progetto ha .sncp/ locale\n- Nessuna sincronizzazione tra progetti\n- Decisioni prese in un progetto non visibili in altri\n- Problema: \"Chi sono?\" e \"Cosa ho imparato?\" si perde tra sessioni/progetti\n\nOpzioni architetturali:\n\nA. Symlink filesystem\n   - Pro: Zero infra, offline-first\n   - Contro: Solo locale, no multi-device\n\nB. Database SQLite centralizzato\n   - Pro: Query potenti, relazioni\n   - Contro: Overhead, meno leggibile\n\nC. Git repo separato (sncp-central)\n   - Pro: Versionato, backup automatico\n   - Contro: Sync manuale, conflitti merge\n\nD. Cloud sync (iCloud/Dropbox)\n   - Pro: Multi-device, automatico\n   - Contro: Privacy, dipendenza esterna\n\nConstraint:\n- DEVE essere offline-first\n- Privacy: dati sensibili, no cloud pubblico\n- Size: <50MB totale\n- Semplicita: Rafa deve capire e mantenere\n\nTask: Scegli architettura migliore.\n\nOutput richiesto:\n1. Analisi 4 opzioni con score (1-10) per ogni criterio\n2. Scelta finale motivata (PERCHE)\n3. Schema/diagramma architettura\n4. Implementation plan (fasi)\n5. Migration plan (da stato attuale)\n6. Risk e mitigation\n7. Success criteria\"\"\",\n        \"pass_threshold\": 0.70\n    }\n]\n\nprint(f\"Loaded {len(TASKS)} tasks (T19-T20) - TIER 3 Complex\")\nprint(\"\\nNOTA: Questi task documentano i LIMITI del modello.\")\nprint(\"Il POC e' gia' PASS con Week 1 + Week 2!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference Function (Extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(task_input, system_prompt=SYSTEM_PROMPT, max_new_tokens=2048):\n",
    "    \"\"\"Run inference on a single task.\n",
    "    \n",
    "    Aumentato max_new_tokens a 2048 per task MOLTO complessi.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format messages for Qwen3 chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": task_input}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the assistant response\n",
    "    if \"assistant\" in response.lower():\n",
    "        response = response.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"latency_seconds\": latency,\n",
    "        \"tokens_generated\": len(outputs[0]) - len(inputs[\"input_ids\"][0])\n",
    "    }\n",
    "\n",
    "print(\"Inference function ready! (max_new_tokens=2048 per task complex)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run T19 - Strategic Planning 6 Mesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run T19\n",
    "task = TASKS[0]\n",
    "print(f\"Running: {task['id']} - {task['name']}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Input:\\n{task['input']}\")\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "result_t19 = run_inference(task[\"input\"])\n",
    "\n",
    "print(f\"Response ({result_t19['latency_seconds']:.2f}s):\")\n",
    "print(result_t19[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run T20 - Architettura Major Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run T20\n",
    "task = TASKS[1]\n",
    "print(f\"Running: {task['id']} - {task['name']}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Input:\\n{task['input']}\")\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "result_t20 = run_inference(task[\"input\"])\n",
    "\n",
    "print(f\"Response ({result_t20['latency_seconds']:.2f}s):\")\n",
    "print(result_t20[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results\n",
    "results = [\n",
    "    {\n",
    "        \"task_id\": \"T19\",\n",
    "        \"task_name\": \"Strategic Planning 6 Mesi\",\n",
    "        \"output\": result_t19[\"response\"],\n",
    "        \"latency_seconds\": result_t19[\"latency_seconds\"],\n",
    "        \"tokens_generated\": result_t19[\"tokens_generated\"],\n",
    "        \"pass_threshold\": 0.70,\n",
    "        \"score\": None,\n",
    "        \"passed\": None\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"T20\",\n",
    "        \"task_name\": \"Architettura Major Decision\",\n",
    "        \"output\": result_t20[\"response\"],\n",
    "        \"latency_seconds\": result_t20[\"latency_seconds\"],\n",
    "        \"tokens_generated\": result_t20[\"tokens_generated\"],\n",
    "        \"pass_threshold\": 0.70,\n",
    "        \"score\": None,\n",
    "        \"passed\": None\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Results collected: {len(results)} tasks\")\n",
    "print(f\"\\nT19 latency: {results[0]['latency_seconds']:.2f}s\")\n",
    "print(f\"T20 latency: {results[1]['latency_seconds']:.2f}s\")\n",
    "print(f\"\\nAvg latency: {(results[0]['latency_seconds'] + results[1]['latency_seconds']) / 2:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation\n",
    "\n",
    "**TIER 3 Evaluation Criteria:**\n",
    "\n",
    "| Criterio | Focus |\n",
    "|----------|-------|\n",
    "| Correttezza | Logica sensata, dati plausibili |\n",
    "| Completezza | Copre tutti i punti richiesti |\n",
    "| Stile Cervella | Filosofia, PERCHE, struttura |\n",
    "| Utility | Actionable, decision-ready |\n",
    "\n",
    "**Pass:** Score >= 70%\n",
    "\n",
    "**NOTA:** Documentiamo i GAP, non penalizziamo per limiti del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_task(task_result, correttezza, completezza, stile, utility, gap_notes=\"\"):\n",
    "    \"\"\"Evaluate a task result with gap documentation.\"\"\"\n",
    "    avg_score = (correttezza + completezza + stile + utility) / 4\n",
    "    score_pct = avg_score * 20\n",
    "    passed = score_pct >= (task_result[\"pass_threshold\"] * 100)\n",
    "    \n",
    "    task_result[\"evaluation\"] = {\n",
    "        \"correttezza\": correttezza,\n",
    "        \"completezza\": completezza,\n",
    "        \"stile\": stile,\n",
    "        \"utility\": utility,\n",
    "        \"avg_score\": avg_score,\n",
    "        \"gap_notes\": gap_notes\n",
    "    }\n",
    "    task_result[\"score\"] = score_pct\n",
    "    task_result[\"passed\"] = passed\n",
    "    \n",
    "    status = \"PASS\" if passed else \"CONDITIONAL\"\n",
    "    print(f\"{task_result['task_id']}: {status} ({score_pct:.0f}%) - threshold {task_result['pass_threshold']*100:.0f}%\")\n",
    "    if gap_notes:\n",
    "        print(f\"  GAP: {gap_notes}\")\n",
    "    \n",
    "    return task_result\n",
    "\n",
    "print(\"Evaluation function ready!\")\n",
    "print(\"\\nPer valutare:\")\n",
    "print('results[0] = evaluate_task(results[0], correttezza=4, completezza=3, stile=4, utility=3, gap_notes=\"Manca budget dettagliato\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valuta T19 - Strategic Planning\n",
    "# COMPILA DOPO AVER VISTO L'OUTPUT!\n",
    "\n",
    "# results[0] = evaluate_task(results[0], \n",
    "#     correttezza=?, \n",
    "#     completezza=?, \n",
    "#     stile=?, \n",
    "#     utility=?,\n",
    "#     gap_notes=\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valuta T20 - Architettura Decision\n",
    "# COMPILA DOPO AVER VISTO L'OUTPUT!\n",
    "\n",
    "# results[1] = evaluate_task(results[1], \n",
    "#     correttezza=?, \n",
    "#     completezza=?, \n",
    "#     stile=?, \n",
    "#     utility=?,\n",
    "#     gap_notes=\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "output = {\n",
    "    \"metadata\": {\n",
    "        \"poc_week\": 3,\n",
    "        \"tier\": \"TIER 3 - Complex\",\n",
    "        \"date\": datetime.now().isoformat(),\n",
    "        \"model\": \"Qwen3-4B-Instruct-2507\",\n",
    "        \"quantization\": \"4-bit\",\n",
    "        \"total_tasks\": len(results),\n",
    "        \"note\": \"TIER 3 documenta GAP, non richiesto per PASS POC\"\n",
    "    },\n",
    "    \"poc_summary\": {\n",
    "        \"week1\": {\"result\": \"PASS\", \"score\": \"9/10 (90%)\", \"avg_latency\": \"19.35s\"},\n",
    "        \"week2\": {\"result\": \"PASS\", \"score\": \"8/8 (100%)\", \"avg_score\": \"89.4%\", \"avg_latency\": \"54.83s\"},\n",
    "        \"week3\": {\"result\": \"TBD\", \"tasks\": \"T19-T20\"}\n",
    "    },\n",
    "    \"results\": results\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open(\"week3_results.json\", \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(\"Results saved to week3_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. POC FINAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final POC Summary\n",
    "print(\"=\"*60)\n",
    "print(\"POC CERVELLA BABY - FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n## RISULTATI PER WEEK\\n\")\n",
    "print(\"| Week | Tier | Task | Passed | Avg Score |\")\n",
    "print(\"|------|------|------|--------|-----------|\")\n",
    "print(\"| Week 1 | Simple | T01-T10 | 9/10 | ~85% |\")\n",
    "print(\"| Week 2 | Medium | T11-T18 | 8/8 | 89.4% |\")\n",
    "\n",
    "# Week 3 results\n",
    "w3_passed = sum(1 for r in results if r[\"passed\"] == True)\n",
    "w3_score = sum(r[\"score\"] for r in results if r[\"score\"]) / len([r for r in results if r[\"score\"]]) if any(r[\"score\"] for r in results) else \"TBD\"\n",
    "print(f\"| Week 3 | Complex | T19-T20 | {w3_passed}/2 | {w3_score if isinstance(w3_score, str) else f'{w3_score:.1f}%'} |\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n## GO/NO-GO DECISION FRAMEWORK\\n\")\n",
    "print(\"Criteri per GO:\")\n",
    "print(\"- [x] TIER 1 >= 60% pass (abbiamo 90%)\")\n",
    "print(\"- [x] TIER 2 >= 62.5% pass (abbiamo 100%)\")\n",
    "print(\"- [x] Modello assorbe COSTITUZIONE\")\n",
    "print(\"- [x] Stile Cervella riconoscibile\")\n",
    "print(\"- [ ] TIER 3 documenta GAP (in valutazione)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n## RACCOMANDAZIONE\\n\")\n",
    "print(\"Basandosi su Week 1 + Week 2:\")\n",
    "print(\"\")\n",
    "print(\"  *** GO - Procedere con MVP Hybrid ***\")\n",
    "print(\"\")\n",
    "print(\"Il modello Qwen3-4B ha superato le aspettative:\")\n",
    "print(\"- 17/18 task PASS (94.4%)\")\n",
    "print(\"- COSTITUZIONE assorbita\")\n",
    "print(\"- REGOLA D'ORO applicata autonomamente\")\n",
    "print(\"- Filosofia Cervella integrata\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print('\"La magia ora e\\' con coscienza!\"')\n",
    "print('\"Ultrapassar os proprios limites!\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps (Post-POC)\n",
    "\n",
    "Se GO:\n",
    "1. **MVP Hybrid** - System Prompts + RAG\n",
    "2. **Fine-tuning** - Con dati SNCP (6-12 mesi)\n",
    "3. **Production** - Deploy su Vast.ai/RunPod\n",
    "\n",
    "---\n",
    "\n",
    "*POC Cervella Baby - Week 3 FINALE*\n",
    "*GO/NO-GO Decision: 1 Febbraio 2026*"
   ]
  }
 ]
}