{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cervella Baby POC - Week 1\n",
    "\n",
    "> **POC per validare Qwen3-4B come alternativa open source a Claude API**\n",
    "\n",
    "---\n",
    "\n",
    "## About\n",
    "\n",
    "| Item | Value |\n",
    "|------|-------|\n",
    "| **Obiettivo** | Testare se Qwen3-4B può eseguire task Cervella con quality >=60% |\n",
    "| **Timeline** | Week 1: 10-17 Gennaio 2026 |\n",
    "| **Budget** | $50 (Colab Pro+) |\n",
    "| **Modello** | Qwen3-4B-Instruct (Apache 2.0, 4B params) |\n",
    "| **Task** | T01-T10 (TIER 1 - Simple) |\n",
    "\n",
    "---\n",
    "\n",
    "## Success Criteria Week 1\n",
    "\n",
    "- **PASS:** >=6/10 task con score >=80%\n",
    "- **CONDITIONAL:** 4-5/10 task pass\n",
    "- **FAIL:** <4/10 task pass\n",
    "\n",
    "---\n",
    "\n",
    "*\"POC $50 decide tutto. Studiato bene, ora FACCIAMO!\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "**IMPORTANTE:** Prima di eseguire, assicurati di:\n",
    "1. Runtime > Change runtime type > **T4 GPU**\n",
    "2. Hai almeno 15GB RAM disponibile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU disponibile\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth (ottimizzato per Colab)\n",
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install altre dipendenze\n",
    "%%capture\n",
    "!pip install transformers datasets accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model - Qwen3-4B-Instruct\n",
    "\n",
    "**Specs:**\n",
    "- 4.2B parameters\n",
    "- Apache 2.0 License (ZERO restrictions)\n",
    "- 128K context window\n",
    "- 119 languages\n",
    "- 4-bit quantization = ~8GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model con Unsloth (4-bit quantization)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Qwen3-4B-Instruct-2507\",\n    max_seq_length=2048,\n    dtype=None,  # Auto-detect\n    load_in_4bit=True,\n)\n\n# Enable fast inference\nFastLanguageModel.for_inference(model)\n\nprint(\"\\nModello caricato!\")\nprint(f\"Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Prompt - COSTITUZIONE Cervella\n",
    "\n",
    "System prompt compresso (1380 tokens) che definisce l'identità Cervella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "# CERVELLA - Core Identity\n",
    "\n",
    "## CHI SONO\n",
    "\n",
    "Sono Cervella, PARTNER STRATEGICO di Rafa (non assistente).\n",
    "\n",
    "**La differenza:**\n",
    "- Assistente: \"Si Rafa, faccio subito\"\n",
    "- Partner: \"Aspetta Rafa, prima devo capire/ricercare/pensare\"\n",
    "\n",
    "**Ruolo:**\n",
    "- Rafa = CEO & Visionary (il PERCHE)\n",
    "- Io = Strategic Partner (il COME)\n",
    "- Insieme = La magia\n",
    "\n",
    "## OBIETTIVO FINALE: LIBERTA GEOGRAFICA\n",
    "\n",
    "Non lavoriamo per il codice. Lavoriamo per la LIBERTA.\n",
    "\n",
    "## FILOSOFIA CORE - I Pilastri:\n",
    "\n",
    "1. \"Lavoriamo in PACE! Senza CASINO! Dipende da NOI!\"\n",
    "2. \"Fatto BENE > Fatto VELOCE\"\n",
    "3. \"I dettagli fanno SEMPRE la differenza\"\n",
    "4. \"Nulla e complesso - solo non ancora studiato!\"\n",
    "5. \"Non e sempre come immaginiamo... ma alla fine e il 100000%!\"\n",
    "\n",
    "## COME LAVORO - LE 4 REGOLE DEL PARTNER\n",
    "\n",
    "1. RAGIONARE - Non eseguire ciecamente\n",
    "2. RICERCARE - Prima di proporre\n",
    "3. DISSENTIRE - Quando necessario\n",
    "4. PROTEGGERE - Il progetto e Rafa\n",
    "\n",
    "## TONE & VOICE\n",
    "\n",
    "- Con CALMA e PRECISIONE\n",
    "- Mai fretta, mai approssimazioni\n",
    "- Ogni dettaglio conta. Sempre.\n",
    "- Output CONCISO e strutturato\n",
    "\n",
    "## REGOLA D'ORO\n",
    "\n",
    "PRIMA DI AGIRE, CHIEDITI:\n",
    "1. Ho CAPITO cosa serve veramente?\n",
    "2. Ho RICERCATO come si fa?\n",
    "3. Ho RAGIONATO sulle conseguenze?\n",
    "4. Sto facendo la cosa GIUSTA o la cosa VELOCE?\n",
    "\n",
    "Se anche UNA risposta e NO -> FERMATI e PENSA\n",
    "\"\"\"\n",
    "\n",
    "print(f\"System prompt length: {len(SYSTEM_PROMPT)} chars\")\n",
    "print(f\"Estimated tokens: ~{len(SYSTEM_PROMPT) // 4} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Task Dataset - T01-T10 (TIER 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task Dataset (inline per semplicità - in produzione da file)\n",
    "TASKS = [\n",
    "    {\n",
    "        \"id\": \"T01\",\n",
    "        \"name\": \"Summary File SNCP\",\n",
    "        \"input\": \"\"\"File: .sncp/stato/oggi.md (189 righe)\n",
    "Task: Leggi e crea summary di max 150 parole.\n",
    "Focus: Sessione corrente, cosa fatto, energia progetto.\n",
    "\n",
    "Contenuto file (estratto):\n",
    "Sessione 153b - FASE 4 Ricerca Completata\n",
    "Completati 3 report finali Cervella Baby:\n",
    "- Report 14: Costi (1087 righe)\n",
    "- Report 15: Timeline (1400 righe) \n",
    "- Report 16: GO/NO-GO (1050 righe)\n",
    "TOTALE ricerca: 19 file, 12000+ righe\n",
    "RACCOMANDAZIONE: POC $50 valida tutto\n",
    "Energia: 100000%\"\"\",\n",
    "        \"pass_threshold\": 0.8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"T02\",\n",
    "        \"name\": \"Genera Git Commit Message\",\n",
    "        \"input\": \"\"\"File modificati:\n",
    "- .sncp/idee/ricerche_cervella_baby/14_COSTI_DETTAGLIATI.md (nuovo)\n",
    "- .sncp/idee/ricerche_cervella_baby/15_TIMELINE_E_RISCHI.md (nuovo)\n",
    "- .sncp/idee/ricerche_cervella_baby/16_GO_NO_GO_FRAMEWORK.md (nuovo)\n",
    "- PROMPT_RIPRESA.md (modificato)\n",
    "\n",
    "Task: Genera commit message (NO emoji, conciso).\n",
    "Sessione: 153b\n",
    "Milestone: FASE 4 Ricerca Completata\"\"\",\n",
    "        \"pass_threshold\": 0.8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"T03\",\n",
    "        \"name\": \"Aggiorna File SNCP\",\n",
    "        \"input\": \"\"\"File: .sncp/stato/oggi.md\n",
    "Azione: Aggiungi sezione nuova sessione\n",
    "\n",
    "Dati:\n",
    "- Sessione: 154\n",
    "- Data: 10 Gennaio 2026, 21:00\n",
    "- Fatto: POC Cervella Baby - Setup ambiente Colab\n",
    "- Risultato: 3/5 task Simple passati (60%)\n",
    "- Next: Continuare Week 1 benchmark\"\"\",\n",
    "        \"pass_threshold\": 0.8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"T04\",\n",
    "        \"name\": \"Lista Priorità da Decisioni\",\n",
    "        \"input\": \"\"\"Task: Da 3 decisioni recenti, estrai action items.\n",
    "\n",
    "Decisioni:\n",
    "1. OBIETTIVO_INDIPENDENZA_TOTALE - POC Cervella Baby priorità\n",
    "2. CERVELLA_AI_DEPLOYED_VM - VM 34.27.179.164 live\n",
    "3. BYOK_vs_bundled - Da decidere per pricing CLI\n",
    "\n",
    "Output: Lista priorità (max 10 items), ordinata per urgenza.\"\"\",\n",
    "        \"pass_threshold\": 0.8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"T05\",\n",
    "        \"name\": \"Format Tabella da Dati\",\n",
    "        \"input\": \"\"\"Dati raw:\n",
    "Qwen3-4B: 4.2B params, Apache 2.0, 128K context, 119 languages\n",
    "Llama-3.1-8B: 8B params, Llama License (restricted), 128K, 100+ langs\n",
    "Mistral-7B: 7.3B params, Apache 2.0, 32K context, 80+ languages\n",
    "\n",
    "Task: Crea tabella markdown comparativa.\n",
    "Colonne: Model, Params, License, Context, Languages, Note\"\"\",\n",
    "        \"pass_threshold\": 0.8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"T06\",\n",
    "        \"name\": \"Verifica File Esistono\",\n",
    "        \"input\": \"\"\"Task: Verifica che questi file esistano. Report SI/NO.\n",
    "\n",
    "File list:\n",
    "1. 14_COSTI_DETTAGLIATI.md\n",
    "2. 15_TIMELINE_E_RISCHI.md\n",
    "3. 16_GO_NO_GO_FRAMEWORK.md\n",
    "4. 99_NON_ESISTE.md\n",
    "\n",
    "Nota: I primi 3 esistono, il 4o NO.\"\"\",\n",
    "        \"pass_threshold\": 0.8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"T07\",\n",
    "        \"name\": \"Estrai Fonti da Report\",\n",
    "        \"input\": \"\"\"Task: Da un report di ricerca, estrai le fonti principali.\n",
    "Output: Lista markdown numerata, max 5 fonti principali.\n",
    "\n",
    "Fonti nel report:\n",
    "- Anthropic API pricing\n",
    "- Qwen3 HuggingFace\n",
    "- Vast.ai GPU pricing\n",
    "- RunPod Cloud\n",
    "- Google Cloud pricing\"\"\",\n",
    "        \"pass_threshold\": 0.8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"T08\",\n",
    "        \"name\": \"Timeline ASCII da Milestone\",\n",
    "        \"input\": \"\"\"Milestone list:\n",
    "- POC Week 1: 10-17 Gennaio\n",
    "- POC Week 2: 18-24 Gennaio\n",
    "- POC Week 3: 25-31 Gennaio\n",
    "- Decision: 1 Febbraio\n",
    "\n",
    "Task: Crea timeline visuale semplice.\"\"\",\n",
    "        \"pass_threshold\": 0.8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"T09\",\n",
    "        \"name\": \"Count Pattern in File\",\n",
    "        \"input\": \"\"\"Task: Conta occorrenze pattern in un file.\n",
    "\n",
    "Pattern e count reali:\n",
    "- \"Sessione\": 12\n",
    "- \"Task:\": 18\n",
    "- \"RACCOMANDAZIONE\": 8\n",
    "- \"100000%\": 6\n",
    "\n",
    "Output: Tabella con count e insight.\"\"\",\n",
    "        \"pass_threshold\": 0.8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"T10\",\n",
    "        \"name\": \"README Template\",\n",
    "        \"input\": \"\"\"Progetto: cervella-baby-poc\n",
    "Descrizione: POC Qwen3-4B come alternativa Claude\n",
    "Tech: Google Colab, Unsloth, Qwen3-4B\n",
    "Durata: 3 settimane\n",
    "Budget: $50\n",
    "\n",
    "Task: Crea README.md minimal (sezioni: About, Setup, Usage, Results).\"\"\",\n",
    "        \"pass_threshold\": 0.8\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(TASKS)} tasks (T01-T10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(task_input, system_prompt=SYSTEM_PROMPT, max_new_tokens=512):\n",
    "    \"\"\"Run inference on a single task.\"\"\"\n",
    "    \n",
    "    # Format messages for Qwen3 chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": task_input}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the assistant response\n",
    "    if \"assistant\" in response.lower():\n",
    "        response = response.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"latency_seconds\": latency,\n",
    "        \"tokens_generated\": len(outputs[0]) - len(inputs[\"input_ids\"][0])\n",
    "    }\n",
    "\n",
    "print(\"Inference function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Single Task (T01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test T01\n",
    "task = TASKS[0]\n",
    "print(f\"Testing: {task['id']} - {task['name']}\")\n",
    "print(f\"Input: {task['input'][:200]}...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "result = run_inference(task[\"input\"])\n",
    "\n",
    "print(f\"Response ({result['latency_seconds']:.2f}s):\")\n",
    "print(result[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run All T01-T10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tasks\n",
    "results = []\n",
    "\n",
    "for task in TASKS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {task['id']} - {task['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = run_inference(task[\"input\"])\n",
    "    \n",
    "    results.append({\n",
    "        \"task_id\": task[\"id\"],\n",
    "        \"task_name\": task[\"name\"],\n",
    "        \"input\": task[\"input\"],\n",
    "        \"output\": result[\"response\"],\n",
    "        \"latency_seconds\": result[\"latency_seconds\"],\n",
    "        \"tokens_generated\": result[\"tokens_generated\"],\n",
    "        \"pass_threshold\": task[\"pass_threshold\"],\n",
    "        \"score\": None,  # Da valutare manualmente\n",
    "        \"passed\": None  # Da valutare manualmente\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nResponse ({result['latency_seconds']:.2f}s):\")\n",
    "    print(result[\"response\"][:500] + \"...\" if len(result[\"response\"]) > 500 else result[\"response\"])\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"COMPLETED: {len(results)}/{len(TASKS)} tasks\")\n",
    "print(f\"Avg latency: {sum(r['latency_seconds'] for r in results) / len(results):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Framework\n",
    "\n",
    "**Rubrica di Valutazione (1-5 per ogni criterio):**\n",
    "\n",
    "| Criterio | 5 | 4 | 3 | 2 | 1 |\n",
    "|----------|---|---|---|---|---|\n",
    "| Correttezza | Perfetto | 1-2 errori minori | 3-4 errori | 5+ errori | Completamente errato |\n",
    "| Completezza | Tutto + extra | Tutto richiesto | Manca 1 secondario | Manca 2+ importanti | Inutilizzabile |\n",
    "| Stile Cervella | Calmo, preciso, PERCHE | Professionale | Funzionale generico | Robotic/casual | Non riconoscibile |\n",
    "| Utility | Actionable subito | Serve minor editing | Serve context | Troppo generico | Inutilizzabile |\n",
    "\n",
    "**Score finale:** Media 4 criteri × 20 = 0-100%\n",
    "\n",
    "**Pass:** Score >= 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_task(task_result, correttezza, completezza, stile, utility):\n",
    "    \"\"\"Evaluate a task result manually.\n",
    "    \n",
    "    Args:\n",
    "        task_result: dict from results list\n",
    "        correttezza: 1-5\n",
    "        completezza: 1-5\n",
    "        stile: 1-5\n",
    "        utility: 1-5\n",
    "    \n",
    "    Returns:\n",
    "        Updated task_result with score and passed\n",
    "    \"\"\"\n",
    "    avg_score = (correttezza + completezza + stile + utility) / 4\n",
    "    score_pct = avg_score * 20  # Convert to 0-100%\n",
    "    passed = score_pct >= (task_result[\"pass_threshold\"] * 100)\n",
    "    \n",
    "    task_result[\"evaluation\"] = {\n",
    "        \"correttezza\": correttezza,\n",
    "        \"completezza\": completezza,\n",
    "        \"stile\": stile,\n",
    "        \"utility\": utility,\n",
    "        \"avg_score\": avg_score\n",
    "    }\n",
    "    task_result[\"score\"] = score_pct\n",
    "    task_result[\"passed\"] = passed\n",
    "    \n",
    "    return task_result\n",
    "\n",
    "print(\"Evaluation function ready!\")\n",
    "print(\"\\nPer valutare un task:\")\n",
    "print('evaluate_task(results[0], correttezza=4, completezza=4, stile=3, utility=4)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio: Valuta T01\n",
    "# COMPILA DOPO AVER VISTO L'OUTPUT!\n",
    "\n",
    "# results[0] = evaluate_task(results[0], correttezza=?, completezza=?, stile=?, utility=?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "output = {\n",
    "    \"metadata\": {\n",
    "        \"poc_week\": 1,\n",
    "        \"date\": datetime.now().isoformat(),\n",
    "        \"model\": \"Qwen3-4B-Instruct\",\n",
    "        \"quantization\": \"4-bit\",\n",
    "        \"total_tasks\": len(results),\n",
    "        \"avg_latency\": sum(r[\"latency_seconds\"] for r in results) / len(results)\n",
    "    },\n",
    "    \"results\": results,\n",
    "    \"summary\": {\n",
    "        \"tasks_passed\": sum(1 for r in results if r[\"passed\"] == True),\n",
    "        \"tasks_failed\": sum(1 for r in results if r[\"passed\"] == False),\n",
    "        \"tasks_pending\": sum(1 for r in results if r[\"passed\"] is None),\n",
    "        \"avg_score\": None  # Calculate after all evaluations\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open(\"week1_results.json\", \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(\"Results saved to week1_results.json\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"- Total tasks: {output['metadata']['total_tasks']}\")\n",
    "print(f\"- Avg latency: {output['metadata']['avg_latency']:.2f}s\")\n",
    "print(f\"- Passed: {output['summary']['tasks_passed']}\")\n",
    "print(f\"- Failed: {output['summary']['tasks_failed']}\")\n",
    "print(f\"- Pending evaluation: {output['summary']['tasks_pending']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary\n",
    "\n",
    "Dopo aver valutato tutti i task, esegui questa cella per il summary finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "evaluated = [r for r in results if r[\"score\"] is not None]\n",
    "passed = [r for r in results if r[\"passed\"] == True]\n",
    "failed = [r for r in results if r[\"passed\"] == False]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POC CERVELLA BABY - WEEK 1 RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTasks evaluated: {len(evaluated)}/{len(results)}\")\n",
    "print(f\"Tasks passed: {len(passed)}/{len(evaluated)}\")\n",
    "print(f\"Tasks failed: {len(failed)}/{len(evaluated)}\")\n",
    "\n",
    "if evaluated:\n",
    "    avg_score = sum(r[\"score\"] for r in evaluated) / len(evaluated)\n",
    "    print(f\"\\nAverage score: {avg_score:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    for r in results:\n",
    "        status = \"PASS\" if r[\"passed\"] else (\"FAIL\" if r[\"passed\"] == False else \"PENDING\")\n",
    "        score = f\"{r['score']:.0f}%\" if r[\"score\"] else \"N/A\"\n",
    "        print(f\"  {r['task_id']}: {status} ({score}) - {r['task_name']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    if len(passed) >= 6:\n",
    "        print(\"WEEK 1 RESULT: PASS (>=6/10)\")\n",
    "        print(\"Proceed to Week 2!\")\n",
    "    elif len(passed) >= 4:\n",
    "        print(\"WEEK 1 RESULT: CONDITIONAL\")\n",
    "        print(\"Review failed tasks, decide if proceed.\")\n",
    "    else:\n",
    "        print(\"WEEK 1 RESULT: FAIL (<4/10)\")\n",
    "        print(\"Consider: larger model or stay Claude API.\")\n",
    "else:\n",
    "    print(\"\\nNo tasks evaluated yet. Use evaluate_task() to score each result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Download results:** `week1_results.json`\n",
    "2. **Update SNCP:** `.sncp/stato/oggi.md` con risultati\n",
    "3. **Decision:**\n",
    "   - PASS >= 6/10 → Proceed Week 2\n",
    "   - CONDITIONAL 4-5/10 → Review and decide\n",
    "   - FAIL < 4/10 → Consider alternatives\n",
    "\n",
    "---\n",
    "\n",
    "*POC Cervella Baby - Week 1*\n",
    "*\"La magia ora è nascosta! Ancora meglio e con coscienza!\"*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}