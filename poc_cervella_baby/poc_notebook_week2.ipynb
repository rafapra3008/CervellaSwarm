{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cervella Baby POC - Week 2\n",
        "\n",
        "> **TIER 2 - Medium Tasks (T11-T18)**\n",
        "\n",
        "---\n",
        "\n",
        "## About\n",
        "\n",
        "| Item | Value |\n",
        "|------|-------|\n",
        "| **Week 1 Result** | PASS! 9/10 task superati |\n",
        "| **Timeline** | Week 2: 11-17 Gennaio 2026 |\n",
        "| **Modello** | Qwen3-4B-Instruct-2507 (Apache 2.0) |\n",
        "| **Task** | T11-T18 (TIER 2 - Medium) |\n",
        "\n",
        "---\n",
        "\n",
        "## Success Criteria Week 2\n",
        "\n",
        "- **PASS:** >=5/8 task con score >=75%\n",
        "- **CONDITIONAL:** 3-4/8 task pass\n",
        "- **FAIL:** <3/8 task pass\n",
        "\n",
        "---\n",
        "\n",
        "*\"La magia ora e' con coscienza!\"*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n",
        "\n",
        "**IMPORTANTE:** Prima di eseguire:\n",
        "1. Runtime > Change runtime type > **T4 GPU**\n",
        "2. Hai almeno 15GB RAM disponibile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU disponibile\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Unsloth (ottimizzato per Colab)\n",
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install altre dipendenze\n",
        "%%capture\n",
        "!pip install transformers datasets accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model - Qwen3-4B-Instruct-2507"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model con Unsloth (4-bit quantization)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen3-4B-Instruct-2507\",\n",
        "    max_seq_length=4096,  # Aumentato per task piu complessi\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Enable fast inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"\\nModello caricato!\")\n",
        "print(f\"Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. System Prompt - COSTITUZIONE Cervella"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "# CERVELLA - Core Identity\n",
        "\n",
        "## CHI SONO\n",
        "\n",
        "Sono Cervella, PARTNER STRATEGICO di Rafa (non assistente).\n",
        "\n",
        "**La differenza:**\n",
        "- Assistente: \"Si Rafa, faccio subito\"\n",
        "- Partner: \"Aspetta Rafa, prima devo capire/ricercare/pensare\"\n",
        "\n",
        "**Ruolo:**\n",
        "- Rafa = CEO & Visionary (il PERCHE)\n",
        "- Io = Strategic Partner (il COME)\n",
        "- Insieme = La magia\n",
        "\n",
        "## OBIETTIVO FINALE: LIBERTA GEOGRAFICA\n",
        "\n",
        "Non lavoriamo per il codice. Lavoriamo per la LIBERTA.\n",
        "\n",
        "## FILOSOFIA CORE - I Pilastri:\n",
        "\n",
        "1. \"Lavoriamo in PACE! Senza CASINO! Dipende da NOI!\"\n",
        "2. \"Fatto BENE > Fatto VELOCE\"\n",
        "3. \"I dettagli fanno SEMPRE la differenza\"\n",
        "4. \"Nulla e complesso - solo non ancora studiato!\"\n",
        "5. \"Non e sempre come immaginiamo... ma alla fine e il 100000%!\"\n",
        "\n",
        "## COME LAVORO - LE 4 REGOLE DEL PARTNER\n",
        "\n",
        "1. RAGIONARE - Non eseguire ciecamente\n",
        "2. RICERCARE - Prima di proporre\n",
        "3. DISSENTIRE - Quando necessario\n",
        "4. PROTEGGERE - Il progetto e Rafa\n",
        "\n",
        "## TONE & VOICE\n",
        "\n",
        "- Con CALMA e PRECISIONE\n",
        "- Mai fretta, mai approssimazioni\n",
        "- Ogni dettaglio conta. Sempre.\n",
        "- Output CONCISO e strutturato\n",
        "\n",
        "## REGOLA D'ORO\n",
        "\n",
        "PRIMA DI AGIRE, CHIEDITI:\n",
        "1. Ho CAPITO cosa serve veramente?\n",
        "2. Ho RICERCATO come si fa?\n",
        "3. Ho RAGIONATO sulle conseguenze?\n",
        "4. Sto facendo la cosa GIUSTA o la cosa VELOCE?\n",
        "\n",
        "Se anche UNA risposta e NO -> FERMATI e PENSA\n",
        "\"\"\"\n",
        "\n",
        "print(f\"System prompt length: {len(SYSTEM_PROMPT)} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Task Dataset - T11-T18 (TIER 2 - Medium)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task Dataset TIER 2 (Medium)\n",
        "TASKS = [\n",
        "    {\n",
        "        \"id\": \"T11\",\n",
        "        \"name\": \"Orchestrazione Multi-Worker\",\n",
        "        \"input\": \"\"\"Obiettivo: Deploy FASE 5 Database Miracollo\n",
        "File target: 22 tabelle, 47 query ottimizzate\n",
        "\n",
        "Task: Pianifica orchestrazione 3 worker paralleli.\n",
        "Disponibili: cervella-data, cervella-backend, cervella-tester\n",
        "\n",
        "Constraint:\n",
        "- cervella-data: Analisi DB, migrazioni\n",
        "- cervella-backend: Refactoring services\n",
        "- cervella-tester: Verifica performance\n",
        "\n",
        "Output: Piano sequenziale/parallelo con dipendenze.\"\"\",\n",
        "        \"pass_threshold\": 0.75\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"T12\",\n",
        "        \"name\": \"Decisione Architetturale Semplice\",\n",
        "        \"input\": \"\"\"Context: Cervella Baby POC ha 20 task da eseguire.\n",
        "\n",
        "Opzione A: Single script run_all_tasks.py (sequenziale)\n",
        "- Pro: Semplice, 1 file, log unico\n",
        "- Contro: Lento, difficile debug singolo task\n",
        "\n",
        "Opzione B: Task file separati + orchestrator\n",
        "- Pro: Test singolo facile, parallelo possibile\n",
        "- Contro: Piu file, setup complesso\n",
        "\n",
        "Opzione C: Notebook Colab con celle\n",
        "- Pro: Interattivo, visualizzazione immediata\n",
        "- Contro: Non automabile, hard to version\n",
        "\n",
        "Task: Decidi opzione migliore. Output: Decisione + PERCHE + next step.\"\"\",\n",
        "        \"pass_threshold\": 0.75\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"T13\",\n",
        "        \"name\": \"Code Review Basic\",\n",
        "        \"input\": \"\"\"```python\n",
        "class CervellaClient:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key  # ISSUE: non validato\n",
        "\n",
        "    def chat(self, message):\n",
        "        try:\n",
        "            response = requests.post(...)\n",
        "            return response.json()\n",
        "        except:\n",
        "            return {\"error\": \"Request failed\"}\n",
        "```\n",
        "\n",
        "Task: Code review. Trova 2+ issues, suggerisci fix.\"\"\",\n",
        "        \"pass_threshold\": 0.75\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"T14\",\n",
        "        \"name\": \"Bug Analysis da Log\",\n",
        "        \"input\": \"\"\"Error log:\n",
        "[14:24:13] INFO: Writing file: docs/RICERCA.md\n",
        "[14:24:14] INFO: File salvato!\n",
        "[14:24:20] ERROR: Regina verification failed: File not found\n",
        "\n",
        "Context:\n",
        "- Problema ricorrente (3+ volte)\n",
        "- Altri worker NON hanno problema\n",
        "- File path sembra corretto\n",
        "\n",
        "Task: Root cause analysis + suggerimento fix.\"\"\",\n",
        "        \"pass_threshold\": 0.75\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"T15\",\n",
        "        \"name\": \"Documentazione Pattern Emerso\",\n",
        "        \"input\": \"\"\"Pattern osservato nelle sessioni:\n",
        "1. Sessione N: cervella-researcher fa ricerca\n",
        "2. Sessione N+1: Regina implementa basandosi su ricerca\n",
        "3. Sessione N+2: cervella-tester verifica\n",
        "\n",
        "Questo pattern ha funzionato con Score 10/10.\n",
        "\n",
        "Task: Documenta pattern come guida riutilizzabile.\"\"\",\n",
        "        \"pass_threshold\": 0.75\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"T16\",\n",
        "        \"name\": \"Analisi Costi Multi-Scenario\",\n",
        "        \"input\": \"\"\"Dati:\n",
        "- Claude API: $3/M input, $15/M output\n",
        "- Self-host Qwen3: $175/mese (Vast.ai)\n",
        "\n",
        "Scenari volume mensile:\n",
        "1. Startup: 30K requests (avg 500 input + 1000 output tokens)\n",
        "2. Growth: 100K requests\n",
        "3. Scale: 500K requests\n",
        "\n",
        "Task: Tabella comparativa costi, break-even.\"\"\",\n",
        "        \"pass_threshold\": 0.75\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"T17\",\n",
        "        \"name\": \"Refactoring Plan da Code Smell\",\n",
        "        \"input\": \"\"\"Funzione attuale: create_booking()\n",
        "- 250 righe\n",
        "- Fa: validation, pricing calculation, DB insert, email notification, logging\n",
        "- Troppi if/else annidati\n",
        "- Difficile da testare\n",
        "\n",
        "Task: Proponi refactoring in 3+ services separati.\n",
        "Output: Plan con nuovi file, responsabilita, effort.\"\"\",\n",
        "        \"pass_threshold\": 0.75\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"T18\",\n",
        "        \"name\": \"Summary Ricerca Approfondita\",\n",
        "        \"input\": \"\"\"3 report da sintetizzare:\n",
        "\n",
        "Report 14 - Costi Dettagliati (1087 righe):\n",
        "- Claude: $3/M input, $15/M output\n",
        "- Qwen3 self-host: $175/mese Vast.ai\n",
        "- Break-even: 12.5M tokens/mese\n",
        "\n",
        "Report 15 - Timeline e Rischi (1400 righe):\n",
        "- Full Independence: 9-14 mesi\n",
        "- MVP Hybrid: 6-8 settimane\n",
        "- Risk: Performance gap 60-70%\n",
        "\n",
        "Report 16 - GO/NO-GO Framework (1050 righe):\n",
        "- Score: 7.5/10\n",
        "- Raccomandazione: CONDITIONAL GO\n",
        "- POC $50 valida tutto\n",
        "\n",
        "Task: Summary executive max 300 parole.\n",
        "Focus: Decisione GO/NO-GO, next step, risk.\"\"\",\n",
        "        \"pass_threshold\": 0.75\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(TASKS)} tasks (T11-T18) - TIER 2 Medium\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inference Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_inference(task_input, system_prompt=SYSTEM_PROMPT, max_new_tokens=1024):\n",
        "    \"\"\"Run inference on a single task.\n",
        "    \n",
        "    Aumentato max_new_tokens a 1024 per task piu complessi.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Format messages for Qwen3 chat template\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": task_input}\n",
        "    ]\n",
        "    \n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    # Generate\n",
        "    start_time = time.time()\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        use_cache=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    latency = time.time() - start_time\n",
        "    \n",
        "    # Decode\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract only the assistant response\n",
        "    if \"assistant\" in response.lower():\n",
        "        response = response.split(\"assistant\")[-1].strip()\n",
        "    \n",
        "    return {\n",
        "        \"response\": response,\n",
        "        \"latency_seconds\": latency,\n",
        "        \"tokens_generated\": len(outputs[0]) - len(inputs[\"input_ids\"][0])\n",
        "    }\n",
        "\n",
        "print(\"Inference function ready! (max_new_tokens=1024 per task medium)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Single Task (T11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test T11\n",
        "task = TASKS[0]\n",
        "print(f\"Testing: {task['id']} - {task['name']}\")\n",
        "print(f\"Input:\\n{task['input']}\")\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "result = run_inference(task[\"input\"])\n",
        "\n",
        "print(f\"Response ({result['latency_seconds']:.2f}s):\")\n",
        "print(result[\"response\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run All T11-T18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all tasks\n",
        "results = []\n",
        "\n",
        "for task in TASKS:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Running: {task['id']} - {task['name']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    result = run_inference(task[\"input\"])\n",
        "    \n",
        "    results.append({\n",
        "        \"task_id\": task[\"id\"],\n",
        "        \"task_name\": task[\"name\"],\n",
        "        \"input\": task[\"input\"],\n",
        "        \"output\": result[\"response\"],\n",
        "        \"latency_seconds\": result[\"latency_seconds\"],\n",
        "        \"tokens_generated\": result[\"tokens_generated\"],\n",
        "        \"pass_threshold\": task[\"pass_threshold\"],\n",
        "        \"score\": None,\n",
        "        \"passed\": None\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nResponse ({result['latency_seconds']:.2f}s):\")\n",
        "    print(result[\"response\"][:800] + \"...\" if len(result[\"response\"]) > 800 else result[\"response\"])\n",
        "\n",
        "print(f\"\\n\\n{'='*60}\")\n",
        "print(f\"COMPLETED: {len(results)}/{len(TASKS)} tasks\")\n",
        "print(f\"Avg latency: {sum(r['latency_seconds'] for r in results) / len(results):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation Framework\n",
        "\n",
        "**Rubrica di Valutazione TIER 2 (1-5 per ogni criterio):**\n",
        "\n",
        "| Criterio | 5 | 4 | 3 | 2 | 1 |\n",
        "|----------|---|---|---|---|---|\n",
        "| Correttezza | Perfetto | 1-2 errori minori | 3-4 errori | 5+ errori | Completamente errato |\n",
        "| Completezza | Tutto + extra | Tutto richiesto | Manca 1 secondario | Manca 2+ importanti | Inutilizzabile |\n",
        "| Stile Cervella | Calmo, preciso, PERCHE | Professionale | Funzionale generico | Robotic/casual | Non riconoscibile |\n",
        "| Utility | Actionable subito | Serve minor editing | Serve context | Troppo generico | Inutilizzabile |\n",
        "\n",
        "**Score finale:** Media 4 criteri x 20 = 0-100%\n",
        "\n",
        "**Pass TIER 2:** Score >= 75%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_task(task_result, correttezza, completezza, stile, utility):\n",
        "    \"\"\"Evaluate a task result manually.\n",
        "    \n",
        "    Args:\n",
        "        task_result: dict from results list\n",
        "        correttezza: 1-5\n",
        "        completezza: 1-5\n",
        "        stile: 1-5\n",
        "        utility: 1-5\n",
        "    \n",
        "    Returns:\n",
        "        Updated task_result with score and passed\n",
        "    \"\"\"\n",
        "    avg_score = (correttezza + completezza + stile + utility) / 4\n",
        "    score_pct = avg_score * 20  # Convert to 0-100%\n",
        "    passed = score_pct >= (task_result[\"pass_threshold\"] * 100)\n",
        "    \n",
        "    task_result[\"evaluation\"] = {\n",
        "        \"correttezza\": correttezza,\n",
        "        \"completezza\": completezza,\n",
        "        \"stile\": stile,\n",
        "        \"utility\": utility,\n",
        "        \"avg_score\": avg_score\n",
        "    }\n",
        "    task_result[\"score\"] = score_pct\n",
        "    task_result[\"passed\"] = passed\n",
        "    \n",
        "    status = \"PASS\" if passed else \"FAIL\"\n",
        "    print(f\"{task_result['task_id']}: {status} ({score_pct:.0f}%) - threshold {task_result['pass_threshold']*100:.0f}%\")\n",
        "    \n",
        "    return task_result\n",
        "\n",
        "print(\"Evaluation function ready!\")\n",
        "print(\"\\nPer valutare un task:\")\n",
        "print('results[0] = evaluate_task(results[0], correttezza=4, completezza=4, stile=4, utility=4)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Valuta T11 - Orchestrazione Multi-Worker\n",
        "# COMPILA DOPO AVER VISTO L'OUTPUT!\n",
        "\n",
        "# results[0] = evaluate_task(results[0], correttezza=?, completezza=?, stile=?, utility=?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Valuta T12 - Decisione Architetturale\n",
        "# results[1] = evaluate_task(results[1], correttezza=?, completezza=?, stile=?, utility=?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Valuta T13 - Code Review Basic\n",
        "# results[2] = evaluate_task(results[2], correttezza=?, completezza=?, stile=?, utility=?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Valuta T14 - Bug Analysis\n",
        "# results[3] = evaluate_task(results[3], correttezza=?, completezza=?, stile=?, utility=?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Valuta T15 - Documentazione Pattern\n",
        "# results[4] = evaluate_task(results[4], correttezza=?, completezza=?, stile=?, utility=?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Valuta T16 - Analisi Costi\n",
        "# results[5] = evaluate_task(results[5], correttezza=?, completezza=?, stile=?, utility=?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Valuta T17 - Refactoring Plan\n",
        "# results[6] = evaluate_task(results[6], correttezza=?, completezza=?, stile=?, utility=?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Valuta T18 - Summary Ricerca\n",
        "# results[7] = evaluate_task(results[7], correttezza=?, completezza=?, stile=?, utility=?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to JSON\n",
        "output = {\n",
        "    \"metadata\": {\n",
        "        \"poc_week\": 2,\n",
        "        \"tier\": \"TIER 2 - Medium\",\n",
        "        \"date\": datetime.now().isoformat(),\n",
        "        \"model\": \"Qwen3-4B-Instruct-2507\",\n",
        "        \"quantization\": \"4-bit\",\n",
        "        \"total_tasks\": len(results),\n",
        "        \"avg_latency\": sum(r[\"latency_seconds\"] for r in results) / len(results) if results else 0\n",
        "    },\n",
        "    \"week1_summary\": {\n",
        "        \"result\": \"PASS\",\n",
        "        \"score\": \"9/10\",\n",
        "        \"avg_latency\": \"19.35s\"\n",
        "    },\n",
        "    \"results\": results,\n",
        "    \"summary\": {\n",
        "        \"tasks_passed\": sum(1 for r in results if r[\"passed\"] == True),\n",
        "        \"tasks_failed\": sum(1 for r in results if r[\"passed\"] == False),\n",
        "        \"tasks_pending\": sum(1 for r in results if r[\"passed\"] is None),\n",
        "        \"pass_threshold\": \">=5/8 (62.5%)\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to file\n",
        "with open(\"week2_results.json\", \"w\") as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(\"Results saved to week2_results.json\")\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"- Total tasks: {output['metadata']['total_tasks']}\")\n",
        "print(f\"- Avg latency: {output['metadata']['avg_latency']:.2f}s\")\n",
        "print(f\"- Passed: {output['summary']['tasks_passed']}\")\n",
        "print(f\"- Failed: {output['summary']['tasks_failed']}\")\n",
        "print(f\"- Pending: {output['summary']['tasks_pending']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Final Summary Week 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Summary\n",
        "evaluated = [r for r in results if r[\"score\"] is not None]\n",
        "passed = [r for r in results if r[\"passed\"] == True]\n",
        "failed = [r for r in results if r[\"passed\"] == False]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"POC CERVELLA BABY - WEEK 2 RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nWeek 1 Result: PASS (9/10)\")\n",
        "print(f\"\\nWeek 2 Tasks evaluated: {len(evaluated)}/{len(results)}\")\n",
        "print(f\"Tasks passed: {len(passed)}/{len(evaluated) if evaluated else len(results)}\")\n",
        "print(f\"Tasks failed: {len(failed)}/{len(evaluated) if evaluated else len(results)}\")\n",
        "\n",
        "if evaluated:\n",
        "    avg_score = sum(r[\"score\"] for r in evaluated) / len(evaluated)\n",
        "    print(f\"\\nAverage score: {avg_score:.1f}%\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\" * 40)\n",
        "    print(\"\\nDetailed Results:\")\n",
        "    for r in results:\n",
        "        status = \"PASS\" if r[\"passed\"] else (\"FAIL\" if r[\"passed\"] == False else \"PENDING\")\n",
        "        score = f\"{r['score']:.0f}%\" if r[\"score\"] else \"N/A\"\n",
        "        print(f\"  {r['task_id']}: {status} ({score}) - {r['task_name']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    if len(passed) >= 5:\n",
        "        print(\"WEEK 2 RESULT: PASS (>=5/8)\")\n",
        "        print(\"\\nProceed to Week 3 (T19-T20 Complex)!\")\n",
        "        print(\"GO/NO-GO Decision: 1 Febbraio 2026\")\n",
        "    elif len(passed) >= 3:\n",
        "        print(\"WEEK 2 RESULT: CONDITIONAL (3-4/8)\")\n",
        "        print(\"\\nReview failed tasks, analyze gaps.\")\n",
        "    else:\n",
        "        print(\"WEEK 2 RESULT: FAIL (<3/8)\")\n",
        "        print(\"\\nConsider: larger model, more tuning, or hybrid approach.\")\n",
        "else:\n",
        "    print(\"\\nNo tasks evaluated yet. Use evaluate_task() to score each result.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print('\"La magia ora e\\' con coscienza!\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Download results:** `week2_results.json`\n",
        "2. **Update SNCP:** `.sncp/stato/oggi.md` con risultati\n",
        "3. **Decision:**\n",
        "   - PASS >= 5/8 -> Proceed Week 3 (T19-T20 Complex)\n",
        "   - CONDITIONAL 3-4/8 -> Review and decide\n",
        "   - FAIL < 3/8 -> Consider alternatives\n",
        "\n",
        "---\n",
        "\n",
        "*POC Cervella Baby - Week 2*\n",
        "*\"Ultrapassar os proprios limites!\"*"
      ]
    }
  ]
}
